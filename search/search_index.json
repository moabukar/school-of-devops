{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"School of DevOps Learn DevOps in a simple and fun way. \ud83d\udee0\ufe0f DevOps is a software development approach that combines development (Dev) and operations (Ops) teams to streamline the software delivery process. It promotes collaboration, communication, and automation between these traditionally separate teams. Level 1 Git Linux Networking AWS Level 2 Terrform Docker K8s CICD","title":"Home"},{"location":"#school-of-devops","text":"Learn DevOps in a simple and fun way. \ud83d\udee0\ufe0f DevOps is a software development approach that combines development (Dev) and operations (Ops) teams to streamline the software delivery process. It promotes collaboration, communication, and automation between these traditionally separate teams.","title":"School of DevOps"},{"location":"#level-1","text":"","title":"Level 1"},{"location":"#git","text":"","title":"Git"},{"location":"#linux","text":"","title":"Linux"},{"location":"#networking","text":"","title":"Networking"},{"location":"#aws","text":"","title":"AWS"},{"location":"#level-2","text":"","title":"Level 2"},{"location":"#terrform","text":"","title":"Terrform"},{"location":"#docker","text":"","title":"Docker"},{"location":"#k8s","text":"","title":"K8s"},{"location":"#cicd","text":"","title":"CICD"},{"location":"CODE_OF_CONDUCT/","text":"Code of Conduct template here","title":"Code of Conduct"},{"location":"CODE_OF_CONDUCT/#code-of-conduct-template-here","text":"","title":"Code of Conduct template here"},{"location":"CONTRIBUTING/","text":"Contributing Guidelines Ensure that you adhere to the following guidelines: Should be about principles and concepts that can be applied in any company or individual project. Do not focus on particular tools or tech stack(which usually change over time). Adhere to the Code of Conduct Should be relevant to the roles and responsibilities of DevOps. Should be locally tested (see steps for testing) and well formatted. Building and testing locally Run the following commands to build and view the site locally before opening a PR. python3 -m venv .venv source .venv/bin/activate pip install -r requirements.txt mkdocs build mkdocs serve","title":"Contribute"},{"location":"CONTRIBUTING/#contributing-guidelines","text":"Ensure that you adhere to the following guidelines: Should be about principles and concepts that can be applied in any company or individual project. Do not focus on particular tools or tech stack(which usually change over time). Adhere to the Code of Conduct Should be relevant to the roles and responsibilities of DevOps. Should be locally tested (see steps for testing) and well formatted.","title":"Contributing Guidelines"},{"location":"CONTRIBUTING/#building-and-testing-locally","text":"Run the following commands to build and view the site locally before opening a PR. python3 -m venv .venv source .venv/bin/activate pip install -r requirements.txt mkdocs build mkdocs serve","title":"Building and testing locally"},{"location":"level1/aws/compute/","text":"Compute","title":"Compute"},{"location":"level1/aws/compute/#compute","text":"","title":"Compute"},{"location":"level1/aws/intro/","text":"","title":"Introduction"},{"location":"level1/aws/plan/","text":"Hands-On AWS Syllabus for DevOps Beginners \u2601\ufe0f Module 1: Introduction to AWS and Cloud Computing What is AWS? Overview of AWS and its significance in cloud computing. Understanding the AWS global infrastructure. Cloud Computing Concepts Basics of cloud computing: IaaS, PaaS, SaaS. Public vs. Private vs. Hybrid clouds. Module 2: AWS Core Services AWS Account Setup Setting up an AWS account. Introduction to AWS Management Console. Key AWS Services Overview of essential services: EC2, S3, RDS, VPC. Basics of IAM (Identity and Access Management). Module 3: Compute in AWS Amazon EC2 (Elastic Compute Cloud) Launching and managing EC2 instances. Understanding EC2 pricing and instance types. Auto Scaling and Load Balancing Implementing Auto Scaling groups. Configuring Elastic Load Balancers. Module 4: AWS Storage Solutions Amazon S3 (Simple Storage Service) Working with S3 buckets and objects. Implementing S3 storage classes and lifecycle policies. Amazon EBS (Elastic Block Store) Managing EBS volumes and snapshots. Understanding EBS vs. Instance Store. Module 5: Networking in AWS Amazon VPC (Virtual Private Cloud) Setting up a VPC and subnets. Configuring route tables, internet gateways, and NAT. AWS Networking Services Introduction to Route 53, CloudFront. Basics of AWS Direct Connect and VPN. Module 6: Databases and AWS Amazon RDS (Relational Database Service) Launching and managing RDS instances. Database backups and snapshots. NoSQL and Amazon DynamoDB Basics of DynamoDB. Use cases for NoSQL databases in AWS. Module 7: Automation and Infrastructure as Code AWS CloudFormation Introduction to Infrastructure as Code. Creating and managing CloudFormation templates. AWS Elastic Beanstalk Deploying applications using Elastic Beanstalk. Understanding Beanstalk's architecture and features. Module 8: DevOps Tools in AWS AWS CodeCommit, CodeBuild, CodeDeploy, and CodePipeline Implementing a CI/CD pipeline with AWS Developer Tools. Best practices for DevOps in AWS. Monitoring and Logging Monitoring with Amazon CloudWatch. Centralized logging with AWS CloudTrail and ElasticSearch Service. Module 9: Security and Compliance in AWS AWS Security Best Practices AWS Shared Responsibility Model. Implementing security measures and IAM policies. Compliance and Governance Understanding AWS compliance programs. AWS tools for governance and compliance. Module 10: Advanced Topics and Real-world Scenarios AWS Lambda and Serverless Architecture Introduction to serverless computing with Lambda. Building serverless applications. Project Work and Case Studies Developing a complete AWS project. Analyzing real-world AWS use cases and scenarios.","title":"Syllabus"},{"location":"level1/aws/plan/#hands-on-aws-syllabus-for-devops-beginners","text":"","title":"Hands-On AWS Syllabus for DevOps Beginners \u2601\ufe0f"},{"location":"level1/aws/plan/#module-1-introduction-to-aws-and-cloud-computing","text":"What is AWS? Overview of AWS and its significance in cloud computing. Understanding the AWS global infrastructure. Cloud Computing Concepts Basics of cloud computing: IaaS, PaaS, SaaS. Public vs. Private vs. Hybrid clouds.","title":"Module 1: Introduction to AWS and Cloud Computing"},{"location":"level1/aws/plan/#module-2-aws-core-services","text":"AWS Account Setup Setting up an AWS account. Introduction to AWS Management Console. Key AWS Services Overview of essential services: EC2, S3, RDS, VPC. Basics of IAM (Identity and Access Management).","title":"Module 2: AWS Core Services"},{"location":"level1/aws/plan/#module-3-compute-in-aws","text":"Amazon EC2 (Elastic Compute Cloud) Launching and managing EC2 instances. Understanding EC2 pricing and instance types. Auto Scaling and Load Balancing Implementing Auto Scaling groups. Configuring Elastic Load Balancers.","title":"Module 3: Compute in AWS"},{"location":"level1/aws/plan/#module-4-aws-storage-solutions","text":"Amazon S3 (Simple Storage Service) Working with S3 buckets and objects. Implementing S3 storage classes and lifecycle policies. Amazon EBS (Elastic Block Store) Managing EBS volumes and snapshots. Understanding EBS vs. Instance Store.","title":"Module 4: AWS Storage Solutions"},{"location":"level1/aws/plan/#module-5-networking-in-aws","text":"Amazon VPC (Virtual Private Cloud) Setting up a VPC and subnets. Configuring route tables, internet gateways, and NAT. AWS Networking Services Introduction to Route 53, CloudFront. Basics of AWS Direct Connect and VPN.","title":"Module 5: Networking in AWS"},{"location":"level1/aws/plan/#module-6-databases-and-aws","text":"Amazon RDS (Relational Database Service) Launching and managing RDS instances. Database backups and snapshots. NoSQL and Amazon DynamoDB Basics of DynamoDB. Use cases for NoSQL databases in AWS.","title":"Module 6: Databases and AWS"},{"location":"level1/aws/plan/#module-7-automation-and-infrastructure-as-code","text":"AWS CloudFormation Introduction to Infrastructure as Code. Creating and managing CloudFormation templates. AWS Elastic Beanstalk Deploying applications using Elastic Beanstalk. Understanding Beanstalk's architecture and features.","title":"Module 7: Automation and Infrastructure as Code"},{"location":"level1/aws/plan/#module-8-devops-tools-in-aws","text":"AWS CodeCommit, CodeBuild, CodeDeploy, and CodePipeline Implementing a CI/CD pipeline with AWS Developer Tools. Best practices for DevOps in AWS. Monitoring and Logging Monitoring with Amazon CloudWatch. Centralized logging with AWS CloudTrail and ElasticSearch Service.","title":"Module 8: DevOps Tools in AWS"},{"location":"level1/aws/plan/#module-9-security-and-compliance-in-aws","text":"AWS Security Best Practices AWS Shared Responsibility Model. Implementing security measures and IAM policies. Compliance and Governance Understanding AWS compliance programs. AWS tools for governance and compliance.","title":"Module 9: Security and Compliance in AWS"},{"location":"level1/aws/plan/#module-10-advanced-topics-and-real-world-scenarios","text":"AWS Lambda and Serverless Architecture Introduction to serverless computing with Lambda. Building serverless applications. Project Work and Case Studies Developing a complete AWS project. Analyzing real-world AWS use cases and scenarios.","title":"Module 10: Advanced Topics and Real-world Scenarios"},{"location":"level1/git/branches/","text":"Working With Branches Coming back to our local repo which has two commits. So far, what we have is a single line of history. Commits are chained in a single line. But sometimes you may have a need to work on two different features in parallel in the same repo. Now one option here could be making a new folder/repo with the same code and use that for another feature development. But there's a better way. Use branches. Since git follows tree like structure for commits, we can use branches to work on different sets of features. From a commit, two or more branches can be created and branches can also be merged. Using branches, there can exist multiple lines of histories and we can checkout to any of them and work on it. Checking out, as we discussed earlier, would simply mean replacing contents of the directory (repo) with the snapshot at the checked out version. Let's create a branch and see how it looks like: $ git branch b1 $ git log --oneline --graph * 7f3b00e ( HEAD -> master, b1 ) adding file 2 * df2fb7a adding file 1 We create a branch called b1 . Git log tells us that b1 also points to the last commit (7f3b00e) but the HEAD is still pointing to master. If you remember, HEAD points to the commit/reference wherever you are checkout to. So if we checkout to b1 , HEAD should point to that. Let's confirm: $ git checkout b1 Switched to branch 'b1' $ git log --oneline --graph * 7f3b00e ( HEAD -> b1, master ) adding file 2 * df2fb7a adding file 1 b1 still points to the same commit but HEAD now points to b1 . Since we create a branch at commit 7f3b00e , there will be two lines of histories starting this commit. Depending on which branch you are checked out on, the line of history will progress. At this moment, we are checked out on branch b1 , so making a new commit will advance branch reference b1 to that commit and current b1 commit will become its parent. Let's do that. # Creating a file and making a commit $ echo \"I am a file in b1 branch\" > b1.txt $ git add b1.txt $ git commit -m \"adding b1 file\" [ b1 872a38f ] adding b1 file 1 file changed, 1 insertion ( + ) create mode 100644 b1.txt # The new line of history $ git log --oneline --graph * 872a38f ( HEAD -> b1 ) adding b1 file * 7f3b00e ( master ) adding file 2 * df2fb7a adding file 1 $ Do note that master is still pointing to the old commit it was pointing to. We can now checkout to master branch and make commits there. This will result in another line of history starting from commit 7f3b00e. # checkout to master branch $ git checkout master Switched to branch 'master' # Creating a new commit on master branch $ echo \"new file in master branch\" > master.txt $ git add master.txt $ git commit -m \"adding master.txt file\" [ master 60dc441 ] adding master.txt file 1 file changed, 1 insertion ( + ) create mode 100644 master.txt # The history line $ git log --oneline --graph * 60dc441 ( HEAD -> master ) adding master.txt file * 7f3b00e adding file 2 * df2fb7a adding file 1 Notice how branch b1 is not visible here since we are on the master. Let's try to visualize both to get the whole picture: $ git log --oneline --graph --all * 60dc441 ( HEAD -> master ) adding master.txt file | * 872a38f ( b1 ) adding b1 file | / * 7f3b00e adding file 2 * df2fb7a adding file 1 Above tree structure should make things clear. Notice a clear branch/fork on commit 7f3b00e. This is how we create branches. Now they both are two separate lines of history on which feature development can be done independently. To reiterate, internally, git is just a tree of commits. Branch names (human readable) are pointers to those commits in the tree. We use various git commands to work with the tree structure and references. Git accordingly modifies contents of our repo. Merges Now say the feature you were working on branch b1 is complete and you need to merge it on master branch, where all the final version of code goes. So first you will checkout to branch master and then you pull the latest code from upstream (eg: GitHub). Then you need to merge your code from b1 into master. There could be two ways this can be done. Here is the current history: $ git log --oneline --graph --all * 60dc441 ( HEAD -> master ) adding master.txt file | * 872a38f ( b1 ) adding b1 file | / * 7f3b00e adding file 2 * df2fb7a adding file 1 Option 1: Directly merge the branch. Merging the branch b1 into master will result in a new merge commit. This will merge changes from two different lines of history and create a new commit of the result. $ git merge b1 Merge made by the 'recursive' strategy. b1.txt | 1 + 1 file changed, 1 insertion ( + ) create mode 100644 b1.txt $ git log --oneline --graph --all * 8fc28f9 ( HEAD -> master ) Merge branch 'b1' | \\ | * 872a38f ( b1 ) adding b1 file * | 60dc441 adding master.txt file | / * 7f3b00e adding file 2 * df2fb7a adding file 1 You can see a new merge commit created (8fc28f9). You will be prompted for the commit message. If there are a lot of branches in the repo, this result will end-up with a lot of merge commits. Which looks ugly compared to a single line of history of development. So let's look at an alternative approach First let's reset our last merge and go to the previous state. $ git reset --hard 60dc441 HEAD is now at 60dc441 adding master.txt file $ git log --oneline --graph --all * 60dc441 ( HEAD -> master ) adding master.txt file | * 872a38f ( b1 ) adding b1 file | / * 7f3b00e adding file 2 * df2fb7a adding file 1 Option 2: Rebase. Now, instead of merging two branches which has a similar base (commit: 7f3b00e), let us rebase branch b1 on to current master. What this means is take branch b1 (from commit 7f3b00e to commit 872a38f) and rebase (put them on top of) master (60dc441). # Switch to b1 $ git checkout b1 Switched to branch 'b1' # Rebase (b1 which is current branch) on master $ git rebase master First, rewinding head to replay your work on top of it... Applying: adding b1 file # The result $ git log --oneline --graph --all * 5372c8f ( HEAD -> b1 ) adding b1 file * 60dc441 ( master ) adding master.txt file * 7f3b00e adding file 2 * df2fb7a adding file 1 You can see b1 which had 1 commit. That commit's parent was 7f3b00e . But since we rebase it on master ( 60dc441 ). That becomes the parent now. As a side effect, you also see it has become a single line of history. Now if we were to merge b1 into master , it would simply mean change master to point to 5372c8f which is b1 . Let's try it: # checkout to master since we want to merge code into master $ git checkout master Switched to branch 'master' # the current history, where b1 is based on master $ git log --oneline --graph --all * 5372c8f ( b1 ) adding b1 file * 60dc441 ( HEAD -> master ) adding master.txt file * 7f3b00e adding file 2 * df2fb7a adding file 1 # Performing the merge, notice the \"fast-forward\" message $ git merge b1 Updating 60dc441..5372c8f Fast-forward b1.txt | 1 + 1 file changed, 1 insertion ( + ) create mode 100644 b1.txt # The Result $ git log --oneline --graph --all * 5372c8f ( HEAD -> master, b1 ) adding b1 file * 60dc441 adding master.txt file * 7f3b00e adding file 2 * df2fb7a adding file 1 Now you see both b1 and master are pointing to the same commit. Your code has been merged to the master branch and it can be pushed. Also we have clean line of history! :D","title":"Working With Branches"},{"location":"level1/git/branches/#working-with-branches","text":"Coming back to our local repo which has two commits. So far, what we have is a single line of history. Commits are chained in a single line. But sometimes you may have a need to work on two different features in parallel in the same repo. Now one option here could be making a new folder/repo with the same code and use that for another feature development. But there's a better way. Use branches. Since git follows tree like structure for commits, we can use branches to work on different sets of features. From a commit, two or more branches can be created and branches can also be merged. Using branches, there can exist multiple lines of histories and we can checkout to any of them and work on it. Checking out, as we discussed earlier, would simply mean replacing contents of the directory (repo) with the snapshot at the checked out version. Let's create a branch and see how it looks like: $ git branch b1 $ git log --oneline --graph * 7f3b00e ( HEAD -> master, b1 ) adding file 2 * df2fb7a adding file 1 We create a branch called b1 . Git log tells us that b1 also points to the last commit (7f3b00e) but the HEAD is still pointing to master. If you remember, HEAD points to the commit/reference wherever you are checkout to. So if we checkout to b1 , HEAD should point to that. Let's confirm: $ git checkout b1 Switched to branch 'b1' $ git log --oneline --graph * 7f3b00e ( HEAD -> b1, master ) adding file 2 * df2fb7a adding file 1 b1 still points to the same commit but HEAD now points to b1 . Since we create a branch at commit 7f3b00e , there will be two lines of histories starting this commit. Depending on which branch you are checked out on, the line of history will progress. At this moment, we are checked out on branch b1 , so making a new commit will advance branch reference b1 to that commit and current b1 commit will become its parent. Let's do that. # Creating a file and making a commit $ echo \"I am a file in b1 branch\" > b1.txt $ git add b1.txt $ git commit -m \"adding b1 file\" [ b1 872a38f ] adding b1 file 1 file changed, 1 insertion ( + ) create mode 100644 b1.txt # The new line of history $ git log --oneline --graph * 872a38f ( HEAD -> b1 ) adding b1 file * 7f3b00e ( master ) adding file 2 * df2fb7a adding file 1 $ Do note that master is still pointing to the old commit it was pointing to. We can now checkout to master branch and make commits there. This will result in another line of history starting from commit 7f3b00e. # checkout to master branch $ git checkout master Switched to branch 'master' # Creating a new commit on master branch $ echo \"new file in master branch\" > master.txt $ git add master.txt $ git commit -m \"adding master.txt file\" [ master 60dc441 ] adding master.txt file 1 file changed, 1 insertion ( + ) create mode 100644 master.txt # The history line $ git log --oneline --graph * 60dc441 ( HEAD -> master ) adding master.txt file * 7f3b00e adding file 2 * df2fb7a adding file 1 Notice how branch b1 is not visible here since we are on the master. Let's try to visualize both to get the whole picture: $ git log --oneline --graph --all * 60dc441 ( HEAD -> master ) adding master.txt file | * 872a38f ( b1 ) adding b1 file | / * 7f3b00e adding file 2 * df2fb7a adding file 1 Above tree structure should make things clear. Notice a clear branch/fork on commit 7f3b00e. This is how we create branches. Now they both are two separate lines of history on which feature development can be done independently. To reiterate, internally, git is just a tree of commits. Branch names (human readable) are pointers to those commits in the tree. We use various git commands to work with the tree structure and references. Git accordingly modifies contents of our repo.","title":"Working With Branches"},{"location":"level1/git/branches/#merges","text":"Now say the feature you were working on branch b1 is complete and you need to merge it on master branch, where all the final version of code goes. So first you will checkout to branch master and then you pull the latest code from upstream (eg: GitHub). Then you need to merge your code from b1 into master. There could be two ways this can be done. Here is the current history: $ git log --oneline --graph --all * 60dc441 ( HEAD -> master ) adding master.txt file | * 872a38f ( b1 ) adding b1 file | / * 7f3b00e adding file 2 * df2fb7a adding file 1 Option 1: Directly merge the branch. Merging the branch b1 into master will result in a new merge commit. This will merge changes from two different lines of history and create a new commit of the result. $ git merge b1 Merge made by the 'recursive' strategy. b1.txt | 1 + 1 file changed, 1 insertion ( + ) create mode 100644 b1.txt $ git log --oneline --graph --all * 8fc28f9 ( HEAD -> master ) Merge branch 'b1' | \\ | * 872a38f ( b1 ) adding b1 file * | 60dc441 adding master.txt file | / * 7f3b00e adding file 2 * df2fb7a adding file 1 You can see a new merge commit created (8fc28f9). You will be prompted for the commit message. If there are a lot of branches in the repo, this result will end-up with a lot of merge commits. Which looks ugly compared to a single line of history of development. So let's look at an alternative approach First let's reset our last merge and go to the previous state. $ git reset --hard 60dc441 HEAD is now at 60dc441 adding master.txt file $ git log --oneline --graph --all * 60dc441 ( HEAD -> master ) adding master.txt file | * 872a38f ( b1 ) adding b1 file | / * 7f3b00e adding file 2 * df2fb7a adding file 1 Option 2: Rebase. Now, instead of merging two branches which has a similar base (commit: 7f3b00e), let us rebase branch b1 on to current master. What this means is take branch b1 (from commit 7f3b00e to commit 872a38f) and rebase (put them on top of) master (60dc441). # Switch to b1 $ git checkout b1 Switched to branch 'b1' # Rebase (b1 which is current branch) on master $ git rebase master First, rewinding head to replay your work on top of it... Applying: adding b1 file # The result $ git log --oneline --graph --all * 5372c8f ( HEAD -> b1 ) adding b1 file * 60dc441 ( master ) adding master.txt file * 7f3b00e adding file 2 * df2fb7a adding file 1 You can see b1 which had 1 commit. That commit's parent was 7f3b00e . But since we rebase it on master ( 60dc441 ). That becomes the parent now. As a side effect, you also see it has become a single line of history. Now if we were to merge b1 into master , it would simply mean change master to point to 5372c8f which is b1 . Let's try it: # checkout to master since we want to merge code into master $ git checkout master Switched to branch 'master' # the current history, where b1 is based on master $ git log --oneline --graph --all * 5372c8f ( b1 ) adding b1 file * 60dc441 ( HEAD -> master ) adding master.txt file * 7f3b00e adding file 2 * df2fb7a adding file 1 # Performing the merge, notice the \"fast-forward\" message $ git merge b1 Updating 60dc441..5372c8f Fast-forward b1.txt | 1 + 1 file changed, 1 insertion ( + ) create mode 100644 b1.txt # The Result $ git log --oneline --graph --all * 5372c8f ( HEAD -> master, b1 ) adding b1 file * 60dc441 adding master.txt file * 7f3b00e adding file 2 * df2fb7a adding file 1 Now you see both b1 and master are pointing to the same commit. Your code has been merged to the master branch and it can be pushed. Also we have clean line of history! :D","title":"Merges"},{"location":"level1/git/conclusion/","text":"What next from here? There are a lot of git commands and features which we have not explored here. But with the base built-up, be sure to explore concepts like Cherrypick Squash Amend Stash Reset","title":"Conclusion"},{"location":"level1/git/conclusion/#what-next-from-here","text":"There are a lot of git commands and features which we have not explored here. But with the base built-up, be sure to explore concepts like Cherrypick Squash Amend Stash Reset","title":"What next from here?"},{"location":"level1/git/git-basics/","text":"Git Prerequisites Have Git installed https://git-scm.com/downloads Have taken any git high level tutorial or following LinkedIn learning courses https://www.linkedin.com/learning/git-essential-training-the-basics/ https://www.linkedin.com/learning/git-branches-merges-and-remotes/ The Official Git Docs What to expect from this course As a DevOps Engineer in the field of tech and comp sci, having knowledge of version control tools becomes almost a requirement. While there are a lot of version control tools that exist today like SVN, Mercurial, etc, Git perhaps is the most used one and this course we will be working with Git. While this course does not start with Git 101 and expects basic knowledge of git as a prerequisite, it will reintroduce the git concepts known by you with details covering what is happening under the hood as you execute various git commands. So that next time you run a git command, you will be able to press enter more confidently! What is not covered under this course Advanced usage and specifics of internal implementation details of Git. Course Contents Git Basics Working with Branches Git with Github Hooks Git Basics Though you might be aware already, let's revisit why we need a version control system. As the project grows and multiple developers start working on it, an efficient method for collaboration is warranted. Git helps the team collaborate easily and also maintains the history of the changes happening with the codebase. Creating a Git Repo Any folder can be converted into a git repository. After executing the following command, we will see a .git folder within the folder, which makes our folder a git repository. All the magic that git does, .git folder is the enabler for the same. # creating an empty folder and changing current dir to it $ cd /tmp $ mkdir school-of-devops $ cd school-of-devops/ # initialize a git repo $ git init Initialized empty Git repository in /private/tmp/school-of-devops/.git/ As the output says, an empty git repo has been initialized in our folder. Let's take a look at what is there. $ ls .git/ HEAD config description hooks info objects refs There are a bunch of folders and files in the .git folder. As I said, all these enables git to do its magic. We will look into some of these folders and files. But for now, what we have is an empty git repository. Tracking a File Now as you might already know, let us create a new file in our repo (we will refer to the folder as repo now.) And see git status $ echo \"I am file 1\" > file1.txt $ git status On branch master No commits yet Untracked files: ( use \"git add <file>...\" to include in what will be committed ) file1.txt nothing added to commit but untracked files present ( use \"git add\" to track ) The current git status says No commits yet and there is one untracked file. Since we just created the file, git is not tracking that file. We explicitly need to ask git to track files and folders. (also checkout gitignore ) And how we do that is via git add command as suggested in the above output. Then we go ahead and create a commit. $ git add file1.txt $ git status On branch master No commits yet Changes to be committed: ( use \"git rm --cached <file>...\" to unstage ) new file: file1.txt $ git commit -m \"adding file 1\" [ master ( root-commit ) df2fb7a ] adding file 1 1 file changed, 1 insertion ( + ) create mode 100644 file1.txt Notice how after adding the file, git status says Changes to be committed: . What it means is whatever is listed there, will be included in the next commit. Then we go ahead and create a commit, with an attached messaged via -m . More About a Commit Commit is a snapshot of the repo. Whenever a commit is made, a snapshot of the current state of repo (the folder) is taken and saved. Each commit has a unique ID. ( df2fb7a for the commit we made in the previous step). As we keep adding/changing more and more contents and keep making commits, all those snapshots are stored by git. Again, all this magic happens inside the .git folder. This is where all this snapshot or versions are stored in an efficient manner. Adding More Changes Let us create one more file and commit the change. It would look the same as the previous commit we made. $ echo \"I am file 2\" > file2.txt $ git add file2.txt $ git commit -m \"adding file 2\" [ master 7f3b00e ] adding file 2 1 file changed, 1 insertion ( + ) create mode 100644 file2.txt A new commit with ID 7f3b00e has been created. You can issue git status at any time to see the state of the repository. **IMPORTANT: Note that commit IDs are long string (SHA) but we can refer to a commit by its initial few (8 or more) characters too. We will interchangeably using shorter and longer commit IDs.** Now that we have two commits, let's visualize them: $ git log --oneline --graph * 7f3b00e ( HEAD -> master ) adding file 2 * df2fb7a adding file 1 git log , as the name suggests, prints the log of all the git commits. Here you see two additional arguments, --oneline prints the shorter version of the log, ie: the commit message only and not the person who made the commit and when. --graph prints it in graph format. Now at this moment the commits might look like just one in each line but all commits are stored as a tree like data structure internally by git. That means there can be two or more children commits of a given commit. And not just a single line of commits. We will look more into this part when we get to the Branches section. For now this is our commit history: df2fb7a === > 7f3b00e Are commits really linked? As I just said, the two commits we just made are linked via tree like data structure and we saw how they are linked. But let's actually verify it. Everything in git is an object. Newly created files are stored as an object. Changes to file are stored as an objects and even commits are objects. To view contents of an object we can use the following command with the object's ID. We will take a look at the contents of the second commit $ git cat-file -p 7f3b00e tree ebf3af44d253e5328340026e45a9fa9ae3ea1982 parent df2fb7a61f5d40c1191e0fdeb0fc5d6e7969685a author Sanket Patel <spatel1@linkedin.com> 1603273316 -0700 committer Sanket Patel <spatel1@linkedin.com> 1603273316 -0700 adding file 2 Take a note of parent attribute in the above output. It points to the commit id of the first commit we made. So this proves that they are linked! Additionally you can see the second commit's message in this object. As I said all this magic is enabled by .git folder and the object to which we are looking at also is in that folder. $ ls .git/objects/7f/3b00eaa957815884198e2fdfec29361108d6a9 .git/objects/7f/3b00eaa957815884198e2fdfec29361108d6a9 It is stored in .git/objects/ folder. All the files and changes to them as well are stored in this folder. The Version Control part of Git We already can see two commits (versions) in our git log. One thing a version control tool gives you is ability to browse back and forth in history. For example: some of your users are running an old version of code and they are reporting an issue. In order to debug the issue, you need access to the old code. The one in your current repo is the latest code. In this example, you are working on the second commit (7f3b00e) and someone reported an issue with the code snapshot at commit (df2fb7a). This is how you would get access to the code at any older commit # Current contents, two files present $ ls file1.txt file2.txt # checking out to (an older) commit $ git checkout df2fb7a Note: checking out 'df2fb7a' . You are in 'detached HEAD' state. You can look around, make experimental changes and commit them, and you can discard any commits you make in this state without impacting any branches by performing another checkout. If you want to create a new branch to retain commits you create, you may do so ( now or later ) by using -b with the checkout command again. Example: git checkout -b <new-branch-name> HEAD is now at df2fb7a adding file 1 # checking contents, can verify it has old contents $ ls file1.txt So this is how we would get access to old versions/snapshots. All we need is a reference to that snapshot. Upon executing git checkout ... , what git does for you is use the .git folder, see what was the state of things (files and folders) at that version/reference and replace the contents of current directory with those contents. The then-existing content will no longer be present in the local dir (repo) but we can and will still get access to them because they are tracked via git commit and .git folder has them stored/tracked. Reference I mention in the previous section that we need a reference to the version. By default, git repo is made of tree of commits. And each commit has a unique IDs. But the unique ID is not the only thing we can reference commits via. There are multiple ways to reference commits. For example: HEAD is a reference to current commit. Whatever commit your repo is checked out at, HEAD will point to that. HEAD~1 is reference to previous commit. So while checking out previous version in section above, we could have done git checkout HEAD~1 . Similarly, master is also a reference (to a branch). Since git uses tree like structure to store commits, there of course will be branches. And the default branch is called master . Master (or any branch reference) will point to the latest commit in the branch. Even though we have checked out to the previous commit in out repo, master still points to the latest commit. And we can get back to the latest version by checkout at master reference $ git checkout master Previous HEAD position was df2fb7a adding file 1 Switched to branch 'master' # now we will see latest code, with two files $ ls file1.txt file2.txt Note, instead of master in above command, we could have used commit's ID as well. References and The Magic Let's look at the state of things. Two commits, master and HEAD references are pointing to the latest commit $ git log --oneline --graph * 7f3b00e ( HEAD -> master ) adding file 2 * df2fb7a adding file 1 The magic? Let's examine these files: $ cat .git/refs/heads/master 7f3b00eaa957815884198e2fdfec29361108d6a9 Viola! Where master is pointing to is stored in a file. Whenever git needs to know where master reference is pointing to, or if git needs to update where master points, it just needs to update the file above. So when you create a new commit, a new commit is created on top of the current commit and the master file is updated with the new commit's ID. Similary, for HEAD reference: $ cat .git/HEAD ref: refs/heads/master We can see HEAD is pointing to a reference called refs/heads/master . So HEAD will point where ever the master points. Little Adventure We discussed how git will update the files as we execute commands. But let's try to do it ourselves, by hand, and see what happens. $ git log --oneline --graph * 7f3b00e ( HEAD -> master ) adding file 2 * df2fb7a adding file 1 Now let's change master to point to the previous/first commit. $ echo df2fb7a61f5d40c1191e0fdeb0fc5d6e7969685a > .git/refs/heads/master $ git log --oneline --graph * df2fb7a ( HEAD -> master ) adding file 1 # RESETTING TO ORIGINAL $ echo 7f3b00eaa957815884198e2fdfec29361108d6a9 > .git/refs/heads/master $ git log --oneline --graph * 7f3b00e ( HEAD -> master ) adding file 2 * df2fb7a adding file 1 We just edited the master reference file and now we can see only the first commit in git log. Undoing the change to the file brings the state back to original. Not so much of magic, is it?","title":"Basics"},{"location":"level1/git/git-basics/#git","text":"","title":"Git"},{"location":"level1/git/git-basics/#prerequisites","text":"Have Git installed https://git-scm.com/downloads Have taken any git high level tutorial or following LinkedIn learning courses https://www.linkedin.com/learning/git-essential-training-the-basics/ https://www.linkedin.com/learning/git-branches-merges-and-remotes/ The Official Git Docs","title":"Prerequisites"},{"location":"level1/git/git-basics/#what-to-expect-from-this-course","text":"As a DevOps Engineer in the field of tech and comp sci, having knowledge of version control tools becomes almost a requirement. While there are a lot of version control tools that exist today like SVN, Mercurial, etc, Git perhaps is the most used one and this course we will be working with Git. While this course does not start with Git 101 and expects basic knowledge of git as a prerequisite, it will reintroduce the git concepts known by you with details covering what is happening under the hood as you execute various git commands. So that next time you run a git command, you will be able to press enter more confidently!","title":"What to expect from this course"},{"location":"level1/git/git-basics/#what-is-not-covered-under-this-course","text":"Advanced usage and specifics of internal implementation details of Git.","title":"What is not covered under this course"},{"location":"level1/git/git-basics/#course-contents","text":"Git Basics Working with Branches Git with Github Hooks","title":"Course Contents"},{"location":"level1/git/git-basics/#git-basics","text":"Though you might be aware already, let's revisit why we need a version control system. As the project grows and multiple developers start working on it, an efficient method for collaboration is warranted. Git helps the team collaborate easily and also maintains the history of the changes happening with the codebase.","title":"Git Basics"},{"location":"level1/git/git-basics/#creating-a-git-repo","text":"Any folder can be converted into a git repository. After executing the following command, we will see a .git folder within the folder, which makes our folder a git repository. All the magic that git does, .git folder is the enabler for the same. # creating an empty folder and changing current dir to it $ cd /tmp $ mkdir school-of-devops $ cd school-of-devops/ # initialize a git repo $ git init Initialized empty Git repository in /private/tmp/school-of-devops/.git/ As the output says, an empty git repo has been initialized in our folder. Let's take a look at what is there. $ ls .git/ HEAD config description hooks info objects refs There are a bunch of folders and files in the .git folder. As I said, all these enables git to do its magic. We will look into some of these folders and files. But for now, what we have is an empty git repository.","title":"Creating a Git Repo"},{"location":"level1/git/git-basics/#tracking-a-file","text":"Now as you might already know, let us create a new file in our repo (we will refer to the folder as repo now.) And see git status $ echo \"I am file 1\" > file1.txt $ git status On branch master No commits yet Untracked files: ( use \"git add <file>...\" to include in what will be committed ) file1.txt nothing added to commit but untracked files present ( use \"git add\" to track ) The current git status says No commits yet and there is one untracked file. Since we just created the file, git is not tracking that file. We explicitly need to ask git to track files and folders. (also checkout gitignore ) And how we do that is via git add command as suggested in the above output. Then we go ahead and create a commit. $ git add file1.txt $ git status On branch master No commits yet Changes to be committed: ( use \"git rm --cached <file>...\" to unstage ) new file: file1.txt $ git commit -m \"adding file 1\" [ master ( root-commit ) df2fb7a ] adding file 1 1 file changed, 1 insertion ( + ) create mode 100644 file1.txt Notice how after adding the file, git status says Changes to be committed: . What it means is whatever is listed there, will be included in the next commit. Then we go ahead and create a commit, with an attached messaged via -m .","title":"Tracking a File"},{"location":"level1/git/git-basics/#more-about-a-commit","text":"Commit is a snapshot of the repo. Whenever a commit is made, a snapshot of the current state of repo (the folder) is taken and saved. Each commit has a unique ID. ( df2fb7a for the commit we made in the previous step). As we keep adding/changing more and more contents and keep making commits, all those snapshots are stored by git. Again, all this magic happens inside the .git folder. This is where all this snapshot or versions are stored in an efficient manner.","title":"More About a Commit"},{"location":"level1/git/git-basics/#adding-more-changes","text":"Let us create one more file and commit the change. It would look the same as the previous commit we made. $ echo \"I am file 2\" > file2.txt $ git add file2.txt $ git commit -m \"adding file 2\" [ master 7f3b00e ] adding file 2 1 file changed, 1 insertion ( + ) create mode 100644 file2.txt A new commit with ID 7f3b00e has been created. You can issue git status at any time to see the state of the repository. **IMPORTANT: Note that commit IDs are long string (SHA) but we can refer to a commit by its initial few (8 or more) characters too. We will interchangeably using shorter and longer commit IDs.** Now that we have two commits, let's visualize them: $ git log --oneline --graph * 7f3b00e ( HEAD -> master ) adding file 2 * df2fb7a adding file 1 git log , as the name suggests, prints the log of all the git commits. Here you see two additional arguments, --oneline prints the shorter version of the log, ie: the commit message only and not the person who made the commit and when. --graph prints it in graph format. Now at this moment the commits might look like just one in each line but all commits are stored as a tree like data structure internally by git. That means there can be two or more children commits of a given commit. And not just a single line of commits. We will look more into this part when we get to the Branches section. For now this is our commit history: df2fb7a === > 7f3b00e","title":"Adding More Changes"},{"location":"level1/git/git-basics/#are-commits-really-linked","text":"As I just said, the two commits we just made are linked via tree like data structure and we saw how they are linked. But let's actually verify it. Everything in git is an object. Newly created files are stored as an object. Changes to file are stored as an objects and even commits are objects. To view contents of an object we can use the following command with the object's ID. We will take a look at the contents of the second commit $ git cat-file -p 7f3b00e tree ebf3af44d253e5328340026e45a9fa9ae3ea1982 parent df2fb7a61f5d40c1191e0fdeb0fc5d6e7969685a author Sanket Patel <spatel1@linkedin.com> 1603273316 -0700 committer Sanket Patel <spatel1@linkedin.com> 1603273316 -0700 adding file 2 Take a note of parent attribute in the above output. It points to the commit id of the first commit we made. So this proves that they are linked! Additionally you can see the second commit's message in this object. As I said all this magic is enabled by .git folder and the object to which we are looking at also is in that folder. $ ls .git/objects/7f/3b00eaa957815884198e2fdfec29361108d6a9 .git/objects/7f/3b00eaa957815884198e2fdfec29361108d6a9 It is stored in .git/objects/ folder. All the files and changes to them as well are stored in this folder.","title":"Are commits really linked?"},{"location":"level1/git/git-basics/#the-version-control-part-of-git","text":"We already can see two commits (versions) in our git log. One thing a version control tool gives you is ability to browse back and forth in history. For example: some of your users are running an old version of code and they are reporting an issue. In order to debug the issue, you need access to the old code. The one in your current repo is the latest code. In this example, you are working on the second commit (7f3b00e) and someone reported an issue with the code snapshot at commit (df2fb7a). This is how you would get access to the code at any older commit # Current contents, two files present $ ls file1.txt file2.txt # checking out to (an older) commit $ git checkout df2fb7a Note: checking out 'df2fb7a' . You are in 'detached HEAD' state. You can look around, make experimental changes and commit them, and you can discard any commits you make in this state without impacting any branches by performing another checkout. If you want to create a new branch to retain commits you create, you may do so ( now or later ) by using -b with the checkout command again. Example: git checkout -b <new-branch-name> HEAD is now at df2fb7a adding file 1 # checking contents, can verify it has old contents $ ls file1.txt So this is how we would get access to old versions/snapshots. All we need is a reference to that snapshot. Upon executing git checkout ... , what git does for you is use the .git folder, see what was the state of things (files and folders) at that version/reference and replace the contents of current directory with those contents. The then-existing content will no longer be present in the local dir (repo) but we can and will still get access to them because they are tracked via git commit and .git folder has them stored/tracked.","title":"The Version Control part of Git"},{"location":"level1/git/git-basics/#reference","text":"I mention in the previous section that we need a reference to the version. By default, git repo is made of tree of commits. And each commit has a unique IDs. But the unique ID is not the only thing we can reference commits via. There are multiple ways to reference commits. For example: HEAD is a reference to current commit. Whatever commit your repo is checked out at, HEAD will point to that. HEAD~1 is reference to previous commit. So while checking out previous version in section above, we could have done git checkout HEAD~1 . Similarly, master is also a reference (to a branch). Since git uses tree like structure to store commits, there of course will be branches. And the default branch is called master . Master (or any branch reference) will point to the latest commit in the branch. Even though we have checked out to the previous commit in out repo, master still points to the latest commit. And we can get back to the latest version by checkout at master reference $ git checkout master Previous HEAD position was df2fb7a adding file 1 Switched to branch 'master' # now we will see latest code, with two files $ ls file1.txt file2.txt Note, instead of master in above command, we could have used commit's ID as well.","title":"Reference"},{"location":"level1/git/git-basics/#references-and-the-magic","text":"Let's look at the state of things. Two commits, master and HEAD references are pointing to the latest commit $ git log --oneline --graph * 7f3b00e ( HEAD -> master ) adding file 2 * df2fb7a adding file 1 The magic? Let's examine these files: $ cat .git/refs/heads/master 7f3b00eaa957815884198e2fdfec29361108d6a9 Viola! Where master is pointing to is stored in a file. Whenever git needs to know where master reference is pointing to, or if git needs to update where master points, it just needs to update the file above. So when you create a new commit, a new commit is created on top of the current commit and the master file is updated with the new commit's ID. Similary, for HEAD reference: $ cat .git/HEAD ref: refs/heads/master We can see HEAD is pointing to a reference called refs/heads/master . So HEAD will point where ever the master points.","title":"References and The Magic"},{"location":"level1/git/git-basics/#little-adventure","text":"We discussed how git will update the files as we execute commands. But let's try to do it ourselves, by hand, and see what happens. $ git log --oneline --graph * 7f3b00e ( HEAD -> master ) adding file 2 * df2fb7a adding file 1 Now let's change master to point to the previous/first commit. $ echo df2fb7a61f5d40c1191e0fdeb0fc5d6e7969685a > .git/refs/heads/master $ git log --oneline --graph * df2fb7a ( HEAD -> master ) adding file 1 # RESETTING TO ORIGINAL $ echo 7f3b00eaa957815884198e2fdfec29361108d6a9 > .git/refs/heads/master $ git log --oneline --graph * 7f3b00e ( HEAD -> master ) adding file 2 * df2fb7a adding file 1 We just edited the master reference file and now we can see only the first commit in git log. Undoing the change to the file brings the state back to original. Not so much of magic, is it?","title":"Little Adventure"},{"location":"level1/git/github-hooks/","text":"Git with GitHub Till now all the operations we did were in our local repo while git also helps us in a collaborative environment. GitHub is one place on the internet where you can centrally host your git repos and collaborate with other developers. Most of the workflow will remain the same as we discussed, with addition of couple of things: Pull: to pull latest changes from github (the central) repo Push: to push your changes to github repo so that it's available to all people GitHub has written nice guides and tutorials about this and you can refer them here: GitHub Hello World Git Handbook Hooks Git has another nice feature called hooks. Hooks are basically scripts which will be called when a certain event happens. Here is where hooks are located: $ ls .git/hooks/ applypatch-msg.sample fsmonitor-watchman.sample pre-applypatch.sample pre-push.sample pre-receive.sample update.sample commit-msg.sample post-update.sample pre-commit.sample pre-rebase.sample prepare-commit-msg.sample Names are self explanatory. These hooks are useful when you want to do certain things when a certain event happens. If you want to run tests before pushing code, you would want to setup pre-push hooks. Let's try to create a pre commit hook. $ echo \"echo this is from pre commit hook\" > .git/hooks/pre-commit $ chmod +x .git/hooks/pre-commit We basically create a file called pre-commit in hooks folder and make it executable. Now if we make a commit, we should see the message getting printed. $ echo \"sample file\" > sample.txt $ git add sample.txt $ git commit -m \"adding sample file\" this is from pre commit hook # <===== THE MESSAGE FROM HOOK EXECUTION [ master 9894e05 ] adding sample file 1 file changed, 1 insertion ( + ) create mode 100644 sample.txt","title":"Github and Hooks"},{"location":"level1/git/github-hooks/#git-with-github","text":"Till now all the operations we did were in our local repo while git also helps us in a collaborative environment. GitHub is one place on the internet where you can centrally host your git repos and collaborate with other developers. Most of the workflow will remain the same as we discussed, with addition of couple of things: Pull: to pull latest changes from github (the central) repo Push: to push your changes to github repo so that it's available to all people GitHub has written nice guides and tutorials about this and you can refer them here: GitHub Hello World Git Handbook","title":"Git with GitHub"},{"location":"level1/git/github-hooks/#hooks","text":"Git has another nice feature called hooks. Hooks are basically scripts which will be called when a certain event happens. Here is where hooks are located: $ ls .git/hooks/ applypatch-msg.sample fsmonitor-watchman.sample pre-applypatch.sample pre-push.sample pre-receive.sample update.sample commit-msg.sample post-update.sample pre-commit.sample pre-rebase.sample prepare-commit-msg.sample Names are self explanatory. These hooks are useful when you want to do certain things when a certain event happens. If you want to run tests before pushing code, you would want to setup pre-push hooks. Let's try to create a pre commit hook. $ echo \"echo this is from pre commit hook\" > .git/hooks/pre-commit $ chmod +x .git/hooks/pre-commit We basically create a file called pre-commit in hooks folder and make it executable. Now if we make a commit, we should see the message getting printed. $ echo \"sample file\" > sample.txt $ git add sample.txt $ git commit -m \"adding sample file\" this is from pre commit hook # <===== THE MESSAGE FROM HOOK EXECUTION [ master 9894e05 ] adding sample file 1 file changed, 1 insertion ( + ) create mode 100644 sample.txt","title":"Hooks"},{"location":"level1/linux/basics/","text":"Linux Basics Introduction What to expect from this course Fundamentals of Linux operating systems: Covers Linux architecture, distributions, and various uses. Explains the difference between GUI and CLI. Basic Linux commands: Focuses on navigating the file system, manipulating files, and using I/O redirection. Linux system administration: Covers tasks performed by Linux admins, such as managing users/groups, file permissions, monitoring system performance, and working with log files. What are Linux operating systems Linux operating systems are a family of open-source operating systems based on the Linux kernel. They are known for their stability, security, and flexibility. Linux distributions, also known as \"distros,\" are different variants of Linux that are customized and packaged with various software applications, tools, and desktop environments. Some popular Linux operating systems include Ubuntu, Fedora, Debian, CentOS, and Arch Linux. These operating systems are widely used in servers, desktop computers, embedded systems, and other devices. A kernel is the most important part of an operating system - it performs important functions like process management, memory management, filesystem management etc. What are popular Linux distributions A Linux distribution(distro) is an operating system based on the Linux kernel and a package management system. A package management system consists of tools that help in installing, upgrading, configuring and removing softwares on the operating system. Software are usually adopted to a distribution and are packaged in a distro specific format. These packages are available through a distro specific repository. Packages are installed and managed in the operating system by a package manager. ** Popular Linux distributions: ** Fedora Ubuntu Debian Centos Red Hat Enterprise Linux Suse Arch Linux Linux Architecture The Linux kernel is monolithic in nature. The Linux kernel is designed as a monolithic kernel, which means that the entire operating system kernel is present and runs in a single address space. This design allows for efficient communication and sharing of data between different kernel components. However, it also means that any issue or bug within the kernel can potentially affect the stability and security of the entire system. System calls are used to interact with the Linux kernel space. System calls are the interface between user space and kernel space in Linux. They provide a way for user programs to request services from the kernel, such as file operations, process management, and device control. When a user program needs to perform a privileged operation or access hardware resources, it makes a system call, which transfers control from user space to kernel space. Kernel code can only be executed in the kernel mode. Non-kernel code is executed in the user mode. The Linux kernel enforces a separation between kernel mode and user mode. Kernel code runs in a privileged mode called kernel mode, where it has direct access to system resources and can execute privileged instructions. On the other hand, non-kernel code, such as user programs, runs in user mode, which restricts their access to system resources and ensures protection and isolation. Device drivers are used to communicate with the hardware devices. Device drivers are software components that facilitate communication between the operating system and hardware devices. In the Linux ecosystem, device drivers are used to control and manage various hardware components, such as network cards, graphics cards, storage devices, and input/output devices. They provide an abstraction layer that allows the operating system to interact with the specific functionalities and features of each hardware device, enabling proper utilization and integration of hardware resources within the system.","title":"Basics"},{"location":"level1/linux/basics/#linux-basics","text":"","title":"Linux Basics"},{"location":"level1/linux/basics/#introduction","text":"","title":"Introduction"},{"location":"level1/linux/basics/#what-to-expect-from-this-course","text":"Fundamentals of Linux operating systems: Covers Linux architecture, distributions, and various uses. Explains the difference between GUI and CLI. Basic Linux commands: Focuses on navigating the file system, manipulating files, and using I/O redirection. Linux system administration: Covers tasks performed by Linux admins, such as managing users/groups, file permissions, monitoring system performance, and working with log files.","title":"What to expect from this course"},{"location":"level1/linux/basics/#what-are-linux-operating-systems","text":"Linux operating systems are a family of open-source operating systems based on the Linux kernel. They are known for their stability, security, and flexibility. Linux distributions, also known as \"distros,\" are different variants of Linux that are customized and packaged with various software applications, tools, and desktop environments. Some popular Linux operating systems include Ubuntu, Fedora, Debian, CentOS, and Arch Linux. These operating systems are widely used in servers, desktop computers, embedded systems, and other devices. A kernel is the most important part of an operating system - it performs important functions like process management, memory management, filesystem management etc.","title":"What are Linux operating systems"},{"location":"level1/linux/basics/#what-are-popular-linux-distributions","text":"A Linux distribution(distro) is an operating system based on the Linux kernel and a package management system. A package management system consists of tools that help in installing, upgrading, configuring and removing softwares on the operating system. Software are usually adopted to a distribution and are packaged in a distro specific format. These packages are available through a distro specific repository. Packages are installed and managed in the operating system by a package manager. ** Popular Linux distributions: ** Fedora Ubuntu Debian Centos Red Hat Enterprise Linux Suse Arch Linux","title":"What are popular Linux distributions"},{"location":"level1/linux/basics/#linux-architecture","text":"The Linux kernel is monolithic in nature. The Linux kernel is designed as a monolithic kernel, which means that the entire operating system kernel is present and runs in a single address space. This design allows for efficient communication and sharing of data between different kernel components. However, it also means that any issue or bug within the kernel can potentially affect the stability and security of the entire system. System calls are used to interact with the Linux kernel space. System calls are the interface between user space and kernel space in Linux. They provide a way for user programs to request services from the kernel, such as file operations, process management, and device control. When a user program needs to perform a privileged operation or access hardware resources, it makes a system call, which transfers control from user space to kernel space. Kernel code can only be executed in the kernel mode. Non-kernel code is executed in the user mode. The Linux kernel enforces a separation between kernel mode and user mode. Kernel code runs in a privileged mode called kernel mode, where it has direct access to system resources and can execute privileged instructions. On the other hand, non-kernel code, such as user programs, runs in user mode, which restricts their access to system resources and ensures protection and isolation. Device drivers are used to communicate with the hardware devices. Device drivers are software components that facilitate communication between the operating system and hardware devices. In the Linux ecosystem, device drivers are used to control and manage various hardware components, such as network cards, graphics cards, storage devices, and input/output devices. They provide an abstraction layer that allows the operating system to interact with the specific functionalities and features of each hardware device, enabling proper utilization and integration of hardware resources within the system.","title":"Linux Architecture"},{"location":"level1/linux/cli/","text":"Command Line Basics What is a Command A command is a program that tells the operating system to perform specific work. Programs are stored as files in Linux. Therefore, a command is also a file that is stored somewhere on the disk. Commands may also take additional arguments as input from the user. These arguments are called command line arguments. Knowing how to use the commands is important, and there are many ways to get help in Linux, especially for commands. Almost every command will have some form of documentation. Most commands will have a command-line argument -h or --help that will display a reasonable amount of documentation. But the most popular documentation system in Linux is called man pages, short for manual pages. To show the documentation for the cat command using the --help option: cat --help File system hierarchy The Linux file system follows a hierarchical structure, forming a tree-like directory structure with the highest level directory called the root (denoted by / ). The directories within the root directory store various types of files related to the system, applications, and users. Here are some of the key directories and their purposes: /bin : This directory contains executable programs for commonly used commands. /dev : Files related to devices on the system are stored in this directory. /etc : System configuration files are stored in this directory. /home : User-related files and directories are stored in this directory. /lib : Library files are stored in this directory. /mnt : Files related to mounted devices on the system can be found in this directory. /proc : This directory contains files related to running processes on the system. /root : User-related files and directories for the root user are stored here. /sbin : Programs used for system administration purposes are stored in this directory. /tmp : This directory is used to store temporary files on the system. /usr : Application programs are stored in this directory. Please note that this is just a brief overview of the file system organization in Linux. Each directory serves a specific purpose and contains various files and subdirectories relevant to that purpose. Understanding the file system structure is essential for navigating and managing files and directories in Linux. It provides a foundation for effective file system management and organization. Commands for Navigating the File System The 3 basic commands which are used frequently to navigate the file system: ls cd pwd Let's now try these commands. ls (print working directory) The ls command is used to list the contents of a directory. It will list down all the files and folders present in the given directory. ls cd (change directory) The cd command can be used to change the working directory. Using the command, you can move from one directory to another. cd /etc The above command will go to the /etc directory. pwd (list files and directories) Commands for Manipulating Files The 5 basic commands which are used frequently to manipulate files: touch mkdir mv rm cp Let's now try these commands. touch (create new file) The touch command can be used to create an empty new file. touch file.txt mkdir (create new directories) The mkdir command is used to create directories.You can use ls command to verify that the new directory is created. mkdir directory_name mv (move files and directories) The mv command can either be used to move files or directories from one location to another or it can be used to rename files or directories. Do note that moving files and copying them are very different. When you move the files or directories, the original copy is lost. General usage of the mv command: mv source_path destination_path rm (delete files and directories) The rm command can be used to delete files and directories. It is very important to note that this command permanently deletes the files and directories. It's nearly impossible and rare to recover these files and directories once you have executed the rm command. Run this command carefully. General usage of the rm command: rm file_name cp (copy files and directories) The cp command is used to copy files and directories from one location to another. General usage of the cp command: cp <source_path> <destination_path> Commands for Viewing Files The 5 basic commands that are used frequently to view files: cat head tail more less cat (view content of a file) The most frequenet use of cat command is to print the contents of the file on your output screen. cat file.txt head The head command displays the first 10 lines of the file by default. You can include additional flags to display as many lines as we want from the top. head file.txt Other usage: head -n 10 file.txt ` tail The tail command displays the last 10 lines of the file by default. You can include additional flags to display as many lines as we want from the end of the file. tail file.txt Other usage: tail -n 10 file.txt more The more command displays the contents of a file or a command output, displaying one screen at a time in case the file is large (Eg: log files). It also allows forward navigation and limited backward navigation in the file. more file.txt less The less command is an improved version of more . It allows you to view the contents of a file or the output of a command, one page at a time. less provides various navigation options and features, making it a versatile tool for reading and exploring large files. To use less with a file, simply type: less filename The echo Command in Linux The echo command is one of the simplest commands that is used in the shell. This command is equivalent to what we have in other programming languages. The echo command prints the given input string on the screen.","title":"CLI"},{"location":"level1/linux/cli/#command-line-basics","text":"","title":"Command Line Basics"},{"location":"level1/linux/cli/#what-is-a-command","text":"A command is a program that tells the operating system to perform specific work. Programs are stored as files in Linux. Therefore, a command is also a file that is stored somewhere on the disk. Commands may also take additional arguments as input from the user. These arguments are called command line arguments. Knowing how to use the commands is important, and there are many ways to get help in Linux, especially for commands. Almost every command will have some form of documentation. Most commands will have a command-line argument -h or --help that will display a reasonable amount of documentation. But the most popular documentation system in Linux is called man pages, short for manual pages. To show the documentation for the cat command using the --help option: cat --help","title":"What is a Command"},{"location":"level1/linux/cli/#file-system-hierarchy","text":"The Linux file system follows a hierarchical structure, forming a tree-like directory structure with the highest level directory called the root (denoted by / ). The directories within the root directory store various types of files related to the system, applications, and users. Here are some of the key directories and their purposes: /bin : This directory contains executable programs for commonly used commands. /dev : Files related to devices on the system are stored in this directory. /etc : System configuration files are stored in this directory. /home : User-related files and directories are stored in this directory. /lib : Library files are stored in this directory. /mnt : Files related to mounted devices on the system can be found in this directory. /proc : This directory contains files related to running processes on the system. /root : User-related files and directories for the root user are stored here. /sbin : Programs used for system administration purposes are stored in this directory. /tmp : This directory is used to store temporary files on the system. /usr : Application programs are stored in this directory. Please note that this is just a brief overview of the file system organization in Linux. Each directory serves a specific purpose and contains various files and subdirectories relevant to that purpose. Understanding the file system structure is essential for navigating and managing files and directories in Linux. It provides a foundation for effective file system management and organization.","title":"File system hierarchy"},{"location":"level1/linux/cli/#commands-for-navigating-the-file-system","text":"The 3 basic commands which are used frequently to navigate the file system: ls cd pwd Let's now try these commands.","title":"Commands for Navigating the File System"},{"location":"level1/linux/cli/#ls-print-working-directory","text":"The ls command is used to list the contents of a directory. It will list down all the files and folders present in the given directory. ls","title":"ls (print working directory)"},{"location":"level1/linux/cli/#cd-change-directory","text":"The cd command can be used to change the working directory. Using the command, you can move from one directory to another. cd /etc The above command will go to the /etc directory.","title":"cd (change directory)"},{"location":"level1/linux/cli/#pwd-list-files-and-directories","text":"","title":"pwd (list files and directories)"},{"location":"level1/linux/cli/#commands-for-manipulating-files","text":"The 5 basic commands which are used frequently to manipulate files: touch mkdir mv rm cp Let's now try these commands.","title":"Commands for Manipulating Files"},{"location":"level1/linux/cli/#touch-create-new-file","text":"The touch command can be used to create an empty new file. touch file.txt","title":"touch (create new file)"},{"location":"level1/linux/cli/#mkdir-create-new-directories","text":"The mkdir command is used to create directories.You can use ls command to verify that the new directory is created. mkdir directory_name","title":"mkdir (create new directories)"},{"location":"level1/linux/cli/#mv-move-files-and-directories","text":"The mv command can either be used to move files or directories from one location to another or it can be used to rename files or directories. Do note that moving files and copying them are very different. When you move the files or directories, the original copy is lost. General usage of the mv command: mv source_path destination_path","title":"mv (move files and directories)"},{"location":"level1/linux/cli/#rm-delete-files-and-directories","text":"The rm command can be used to delete files and directories. It is very important to note that this command permanently deletes the files and directories. It's nearly impossible and rare to recover these files and directories once you have executed the rm command. Run this command carefully. General usage of the rm command: rm file_name","title":"rm (delete files and directories)"},{"location":"level1/linux/cli/#cp-copy-files-and-directories","text":"The cp command is used to copy files and directories from one location to another. General usage of the cp command: cp <source_path> <destination_path>","title":"cp (copy files and directories)"},{"location":"level1/linux/cli/#commands-for-viewing-files","text":"The 5 basic commands that are used frequently to view files: cat head tail more less","title":"Commands for Viewing Files"},{"location":"level1/linux/cli/#cat-view-content-of-a-file","text":"The most frequenet use of cat command is to print the contents of the file on your output screen. cat file.txt","title":"cat (view content of a file)"},{"location":"level1/linux/cli/#head","text":"The head command displays the first 10 lines of the file by default. You can include additional flags to display as many lines as we want from the top. head file.txt Other usage: head -n 10 file.txt `","title":"head"},{"location":"level1/linux/cli/#tail","text":"The tail command displays the last 10 lines of the file by default. You can include additional flags to display as many lines as we want from the end of the file. tail file.txt Other usage: tail -n 10 file.txt","title":"tail"},{"location":"level1/linux/cli/#more","text":"The more command displays the contents of a file or a command output, displaying one screen at a time in case the file is large (Eg: log files). It also allows forward navigation and limited backward navigation in the file. more file.txt","title":"more"},{"location":"level1/linux/cli/#less","text":"The less command is an improved version of more . It allows you to view the contents of a file or the output of a command, one page at a time. less provides various navigation options and features, making it a versatile tool for reading and exploring large files. To use less with a file, simply type: less filename","title":"less"},{"location":"level1/linux/cli/#the-echo-command-in-linux","text":"The echo command is one of the simplest commands that is used in the shell. This command is equivalent to what we have in other programming languages. The echo command prints the given input string on the screen.","title":"The echo Command in Linux"},{"location":"level1/networking/basics/","text":"OSI Model (Layer 1 to 7) The OSI (Open Systems Interconnection) model is a conceptual framework used to understand how different network protocols and technologies interact with each other. It consists of seven layers: IP/TCP/UDP IP (Internet Protocol) : IP is the core protocol of the internet, responsible for logical addressing and routing of data packets between different networks. IPv4 and IPv6 are the most widely used versions. TCP (Transmission Control Protocol) : TCP is a reliable, connection-oriented protocol that provides error detection, flow control, and sequencing of data packets. It guarantees the reliable delivery of data. UDP (User Datagram Protocol) : UDP is a lightweight, connectionless protocol that provides faster communication by sacrificing reliability. It is commonly used for real-time applications like streaming and VoIP. DNS DNS (Domain Name System) is a decentralized system that translates human-readable domain names (e.g., example.com) into IP addresses. It enables users to access websites using memorable domain names instead of IP addresses. Firewalls (stateless/stateful) Stateless Firewalls : Stateless firewalls filter network traffic based on predefined rules without considering the context or state of the network connection. They inspect each packet individually. Stateful Firewalls : Stateful firewalls maintain a state table that tracks the state of network connections. They analyze the context of each packet, including its source, destination, and connection status, to make more informed filtering decisions. CIDR/Subnetting CIDR (Classless Inter-Domain Routing) is a method of allocating and addressing IP networks. It allows for more efficient use of IP addresses by using variable-length subnet masks (VLSM) to divide IP address space into smaller subnets. HTTP/HTTPS/SSL/TTL HTTP (Hypertext Transfer Protocol) : HTTP is a protocol used for communication between web browsers and web servers. It defines how clients request resources and servers respond with HTML pages, images, etc. HTTPS (Hypertext Transfer Protocol Secure) : HTTPS is a secure version of HTTP that uses encryption to protect the privacy and integrity of data exchanged between clients and servers. It uses SSL/TLS protocols to establish a secure connection. SSL/TLS (Secure Sockets Layer/Transport Layer Security) : SSL/TLS are cryptographic","title":"Basics"},{"location":"level1/networking/basics/#osi-model-layer-1-to-7","text":"The OSI (Open Systems Interconnection) model is a conceptual framework used to understand how different network protocols and technologies interact with each other. It consists of seven layers:","title":"OSI Model (Layer 1 to 7)"},{"location":"level1/networking/basics/#iptcpudp","text":"IP (Internet Protocol) : IP is the core protocol of the internet, responsible for logical addressing and routing of data packets between different networks. IPv4 and IPv6 are the most widely used versions. TCP (Transmission Control Protocol) : TCP is a reliable, connection-oriented protocol that provides error detection, flow control, and sequencing of data packets. It guarantees the reliable delivery of data. UDP (User Datagram Protocol) : UDP is a lightweight, connectionless protocol that provides faster communication by sacrificing reliability. It is commonly used for real-time applications like streaming and VoIP.","title":"IP/TCP/UDP"},{"location":"level1/networking/basics/#dns","text":"DNS (Domain Name System) is a decentralized system that translates human-readable domain names (e.g., example.com) into IP addresses. It enables users to access websites using memorable domain names instead of IP addresses.","title":"DNS"},{"location":"level1/networking/basics/#firewalls-statelessstateful","text":"Stateless Firewalls : Stateless firewalls filter network traffic based on predefined rules without considering the context or state of the network connection. They inspect each packet individually. Stateful Firewalls : Stateful firewalls maintain a state table that tracks the state of network connections. They analyze the context of each packet, including its source, destination, and connection status, to make more informed filtering decisions.","title":"Firewalls (stateless/stateful)"},{"location":"level1/networking/basics/#cidrsubnetting","text":"CIDR (Classless Inter-Domain Routing) is a method of allocating and addressing IP networks. It allows for more efficient use of IP addresses by using variable-length subnet masks (VLSM) to divide IP address space into smaller subnets.","title":"CIDR/Subnetting"},{"location":"level1/networking/basics/#httphttpssslttl","text":"HTTP (Hypertext Transfer Protocol) : HTTP is a protocol used for communication between web browsers and web servers. It defines how clients request resources and servers respond with HTML pages, images, etc. HTTPS (Hypertext Transfer Protocol Secure) : HTTPS is a secure version of HTTP that uses encryption to protect the privacy and integrity of data exchanged between clients and servers. It uses SSL/TLS protocols to establish a secure connection. SSL/TLS (Secure Sockets Layer/Transport Layer Security) : SSL/TLS are cryptographic","title":"HTTP/HTTPS/SSL/TTL"},{"location":"level1/networking/load-balancers/","text":"balance request between multiple backends on the backend, while a reverse proxy doesn\u2019t have to balance, it makes requests on your behalf (but the RP doesn\u2019t have balancing logic necessarily So every load balancer is a reverse proxy but not every reverse proxy is a load balancer L4 vs L7 Load balancer (aka fault tolerant system) LB/reverse proxy starts multiple TCP connections with each backend server (this is known as warrming up, keeping things hot) - Layer 4 LB (L4 is stateful) when a client connects to the L4 LB, the LB chooses one server and all segments for that connections go that server. That connection will have state and it will tag it to one and only 1 connection coz L4 only deals with ports, IP addresses there\u2019s also something called a NAT mode which makes everything into a single TCP connection TCP connection is taken to reverse proxy then the reverse proxy rewrites it into a brand new TCP connection Pros simpler load balancing (simple coz it doesn\u2019t do with the data and mainly the ports, IP addresses and TCP connections, does not understand L7 content is) efficient (no data lookup, just looks at ports and IPs) more secure (with L7, data needs to be decrypted and encrypted but in L4 none of that is needed coz its end to end) works with any protocol (coz it doesn\u2019t look at the content, its agnostic. you send segments and it sends it back to the server) one TCP connection (NAT) Cons No smart load balancing Not applicable for micro services Sticky per connection (requests only go to one connection per server) No caching (coz you dont know what data is there, cant cache) Protocol unaware (can be dangerous) bypass rules - instead you need to play with ports If you are using L7 and want to support any protocol you use move to L4 but once you move to L4, any L7 LB logic you have like blocking users or any auth methods, block certain headers, you cant do anymore of that and you can just downgrade the protocol using the HTTP UPGRADE method for other protocols. Layer 7 LB same concept, uses TCP connection when a client connects to the L7 LB, it becomes protocol specific. Any logical \u201crequest\u201d will be forwarded to a new backend server. This could be more one or more segments logical requests will be buffered and will be loaded by the L7 LB in the middle the data needs to be read and if it\u2019s encrypted, it needs to decrypt the data and to decypt the data, you need to have a secure connection between you and the LB server that means if you ever want to host a site, the cert has to live in the L7 LB. People dont like that coz your private key has to live in the L7 the LB parses and understands the segments. HTTP is stateless so i can pick another server and this is how the HTTP smuggling attacks happen Pros Smart load balancing (due to paths, diff rules) like you are going to /about, i can take you to this server and if you\u2019re going to /contactus, i can take you to this server etc Caching/CDN - Great for micro services API gateway logic Auth Cons Expensive (looks at data, decrypt, encrypt) Decrypts (decrypting content, terminates the address, so it terminates TLS - thats why its called TLS Terminator) - L7 LB is always called TLS terminator coz it always terminates connection 2 TCP connections (not sure if its really a con) must share TLS cert Needs to buffer (it needs to read the request once it arrives from the client, LB can be a bottleneck and could slow things down) needs to understand the protocol (people always ask NGINX and HAProxy to support different protocols - YOU simply cannot do L7 LB if the LB does not understand the protocol) we need to undestand the protocol coz we are reading the data so we need to know how to read and respond to the data. NGINX What is Nginx? web server serves web content listens on an HTTP endpoint reverse proxy load balancing backend routing Caching API gateway concepts nginx frontend: communication with the client nginx backend: communication with the backend servers L4 and L7 recap L4 (TCP) IP, TCP, ports L7 (HTTP) more context, app info, HTTP req etc TLS Termination Nginx has TLS NGINX terminates TLS, decrypts, optionally rewrite and then re-encrypt the content to the backend need to look at a method of creating certs or vertifying the nginx - nginx can create its own key or cert? TLS passthrough backend is TLS NGINX proxies/streams the packets directly to the backende just like a tunnel - the nginx does not decrypt any data no cahcing, L4 check only but more secure. NGINX doesn\u2019t need the backend cert here. here you cant cache coz the nginx cant see the L7 content anymore. so only pure L4 checks only Nginx architecture","title":"Load balancers"},{"location":"level1/networking/load-balancers/#layer-4-lb-l4-is-stateful","text":"when a client connects to the L4 LB, the LB chooses one server and all segments for that connections go that server. That connection will have state and it will tag it to one and only 1 connection coz L4 only deals with ports, IP addresses there\u2019s also something called a NAT mode which makes everything into a single TCP connection TCP connection is taken to reverse proxy then the reverse proxy rewrites it into a brand new TCP connection Pros simpler load balancing (simple coz it doesn\u2019t do with the data and mainly the ports, IP addresses and TCP connections, does not understand L7 content is) efficient (no data lookup, just looks at ports and IPs) more secure (with L7, data needs to be decrypted and encrypted but in L4 none of that is needed coz its end to end) works with any protocol (coz it doesn\u2019t look at the content, its agnostic. you send segments and it sends it back to the server) one TCP connection (NAT) Cons No smart load balancing Not applicable for micro services Sticky per connection (requests only go to one connection per server) No caching (coz you dont know what data is there, cant cache) Protocol unaware (can be dangerous) bypass rules - instead you need to play with ports If you are using L7 and want to support any protocol you use move to L4 but once you move to L4, any L7 LB logic you have like blocking users or any auth methods, block certain headers, you cant do anymore of that and you can just downgrade the protocol using the HTTP UPGRADE method for other protocols.","title":"Layer 4 LB (L4 is stateful)"},{"location":"level1/networking/load-balancers/#layer-7-lb","text":"same concept, uses TCP connection when a client connects to the L7 LB, it becomes protocol specific. Any logical \u201crequest\u201d will be forwarded to a new backend server. This could be more one or more segments logical requests will be buffered and will be loaded by the L7 LB in the middle the data needs to be read and if it\u2019s encrypted, it needs to decrypt the data and to decypt the data, you need to have a secure connection between you and the LB server that means if you ever want to host a site, the cert has to live in the L7 LB. People dont like that coz your private key has to live in the L7 the LB parses and understands the segments. HTTP is stateless so i can pick another server and this is how the HTTP smuggling attacks happen Pros Smart load balancing (due to paths, diff rules) like you are going to /about, i can take you to this server and if you\u2019re going to /contactus, i can take you to this server etc Caching/CDN - Great for micro services API gateway logic Auth Cons Expensive (looks at data, decrypt, encrypt) Decrypts (decrypting content, terminates the address, so it terminates TLS - thats why its called TLS Terminator) - L7 LB is always called TLS terminator coz it always terminates connection 2 TCP connections (not sure if its really a con) must share TLS cert Needs to buffer (it needs to read the request once it arrives from the client, LB can be a bottleneck and could slow things down) needs to understand the protocol (people always ask NGINX and HAProxy to support different protocols - YOU simply cannot do L7 LB if the LB does not understand the protocol) we need to undestand the protocol coz we are reading the data so we need to know how to read and respond to the data.","title":"Layer 7 LB"},{"location":"level1/networking/load-balancers/#nginx","text":"What is Nginx? web server serves web content listens on an HTTP endpoint reverse proxy load balancing backend routing Caching API gateway concepts nginx frontend: communication with the client nginx backend: communication with the backend servers L4 and L7 recap L4 (TCP) IP, TCP, ports L7 (HTTP) more context, app info, HTTP req etc TLS Termination Nginx has TLS NGINX terminates TLS, decrypts, optionally rewrite and then re-encrypt the content to the backend need to look at a method of creating certs or vertifying the nginx - nginx can create its own key or cert? TLS passthrough backend is TLS NGINX proxies/streams the packets directly to the backende just like a tunnel - the nginx does not decrypt any data no cahcing, L4 check only but more secure. NGINX doesn\u2019t need the backend cert here. here you cant cache coz the nginx cant see the L7 content anymore. so only pure L4 checks only Nginx architecture","title":"NGINX"},{"location":"level1/networking/osi/","text":"OSI Model Explained \ud83c\udf10 The OSI (Open Systems Interconnection) model is a conceptual framework used to understand how different network protocols and technologies interact with each other. It consists of seven layers: 1. Physical Layer \ud83d\udea6 What It Does : Deals with the physical aspects of network connectivity including the specifications of cables, connectors, and network devices. Key Points : Network hardware. Electrical signals. Data transmission. 2. Data Link Layer \ud83c\udf09 What It Does : The data link layer provides reliable and error-free communication between directly connected devices by using protocols like Ethernet. It handles framing, error detection, and flow control. Key Points : Manages direct device connections. Error checking. Data framing. 3. Network Layer \ud83d\uddfa\ufe0f What It Does : The network layer is responsible for logical addressing and routing of data packets. IP (Internet Protocol) operates at this layer and enables the internetworking of different networks. Key Points : IP addressing. Data routing. Packet forwarding. 4. Transport Layer \ud83d\ude9a What It Does : The transport layer ensures reliable and orderly delivery of data between end systems. TCP (Transmission Control Protocol) provides reliable, connection-oriented communication, while UDP (User Datagram Protocol) offers faster, connectionless communication. Key Points : Data transportation. TCP/UDP protocols. Error recovery. 5. Session Layer \ud83e\udd1d What It Does : The session layer establishes, manages, and terminates connections between applications. It provides services such as session establishment, synchronization, and checkpointing. Key Points : Session setup. Session management. Session termination. 6. Presentation Layer \ud83c\udfa8 What It Does : The presentation layer handles data formatting and conversion to ensure compatibility between different systems. It deals with data encryption, compression, and character encoding.. Key Points : Data translation. Encryption. Compression. 7. Application Layer \ud83d\udcf1 What It Does : The application layer is responsible for providing services directly to the end-user applications. Protocols like HTTP, FTP, DNS, and SMTP operate at this layer.. Key Points : Application services. Protocols like HTTP, FTP, DNS, SMTP. Each layer has a specific role, ensuring smooth data flow in a network. \ud83c\udf10\u2728","title":"OSI Model"},{"location":"level1/networking/osi/#osi-model-explained","text":"The OSI (Open Systems Interconnection) model is a conceptual framework used to understand how different network protocols and technologies interact with each other. It consists of seven layers:","title":"OSI Model Explained \ud83c\udf10"},{"location":"level1/networking/osi/#1-physical-layer","text":"What It Does : Deals with the physical aspects of network connectivity including the specifications of cables, connectors, and network devices. Key Points : Network hardware. Electrical signals. Data transmission.","title":"1. Physical Layer \ud83d\udea6"},{"location":"level1/networking/osi/#2-data-link-layer","text":"What It Does : The data link layer provides reliable and error-free communication between directly connected devices by using protocols like Ethernet. It handles framing, error detection, and flow control. Key Points : Manages direct device connections. Error checking. Data framing.","title":"2. Data Link Layer \ud83c\udf09"},{"location":"level1/networking/osi/#3-network-layer","text":"What It Does : The network layer is responsible for logical addressing and routing of data packets. IP (Internet Protocol) operates at this layer and enables the internetworking of different networks. Key Points : IP addressing. Data routing. Packet forwarding.","title":"3. Network Layer \ud83d\uddfa\ufe0f"},{"location":"level1/networking/osi/#4-transport-layer","text":"What It Does : The transport layer ensures reliable and orderly delivery of data between end systems. TCP (Transmission Control Protocol) provides reliable, connection-oriented communication, while UDP (User Datagram Protocol) offers faster, connectionless communication. Key Points : Data transportation. TCP/UDP protocols. Error recovery.","title":"4. Transport Layer \ud83d\ude9a"},{"location":"level1/networking/osi/#5-session-layer","text":"What It Does : The session layer establishes, manages, and terminates connections between applications. It provides services such as session establishment, synchronization, and checkpointing. Key Points : Session setup. Session management. Session termination.","title":"5. Session Layer \ud83e\udd1d"},{"location":"level1/networking/osi/#6-presentation-layer","text":"What It Does : The presentation layer handles data formatting and conversion to ensure compatibility between different systems. It deals with data encryption, compression, and character encoding.. Key Points : Data translation. Encryption. Compression.","title":"6. Presentation Layer \ud83c\udfa8"},{"location":"level1/networking/osi/#7-application-layer","text":"What It Does : The application layer is responsible for providing services directly to the end-user applications. Protocols like HTTP, FTP, DNS, and SMTP operate at this layer.. Key Points : Application services. Protocols like HTTP, FTP, DNS, SMTP. Each layer has a specific role, ensuring smooth data flow in a network. \ud83c\udf10\u2728","title":"7. Application Layer \ud83d\udcf1"},{"location":"level1/networking/plan/","text":"Networking Fundamentals Introduction to Networking \ud83c\udf10 Importance of networking in the context of DevOps and IT operations Overview of key networking concepts and protocols OSI Model (Layer 1 to 7) Explanation of each layer of the OSI model and their functions Examples of protocols and technologies associated with each layer IP/TCP/UDP Overview of IP (Internet Protocol) and its role in network communication Explanation of TCP (Transmission Control Protocol) and UDP (User Datagram Protocol) and their differences Use cases and scenarios where TCP or UDP is preferred DNS (Domain Name System) Introduction to DNS and its importance in resolving domain names to IP addresses DNS record types and their purposes (A, CNAME, MX, etc.) Configuring DNS settings and working with DNS providers Firewalls (Stateless/Stateful) Difference between stateless and stateful firewalls Explanation of firewall rules and their impact on network traffic Practical examples of configuring and managing firewalls CIDR/Subnetting Understanding CIDR (Classless Inter-Domain Routing) and its role in IP address allocation Subnetting and subnet mask calculations Practical exercises and examples of subnetting HTTP/HTTPS/SSL/TLS Overview of HTTP (Hypertext Transfer Protocol) and HTTPS (HTTP Secure) Understanding SSL (Secure Sockets Layer) and TLS (Transport Layer Security) protocols Importance of encryption and secure communication over the web Forward vs Reverse Proxy (Load Balancers) Difference between forward and reverse proxies Use cases and benefits of using load balancers in distributed systems Configuring and managing load balancers for efficient traffic distribution","title":"Syllabus"},{"location":"level1/networking/plan/#networking-fundamentals","text":"","title":"Networking Fundamentals"},{"location":"level1/networking/plan/#introduction-to-networking","text":"Importance of networking in the context of DevOps and IT operations Overview of key networking concepts and protocols","title":"Introduction to Networking \ud83c\udf10"},{"location":"level1/networking/plan/#osi-model-layer-1-to-7","text":"Explanation of each layer of the OSI model and their functions Examples of protocols and technologies associated with each layer","title":"OSI Model (Layer 1 to 7)"},{"location":"level1/networking/plan/#iptcpudp","text":"Overview of IP (Internet Protocol) and its role in network communication Explanation of TCP (Transmission Control Protocol) and UDP (User Datagram Protocol) and their differences Use cases and scenarios where TCP or UDP is preferred","title":"IP/TCP/UDP"},{"location":"level1/networking/plan/#dns-domain-name-system","text":"Introduction to DNS and its importance in resolving domain names to IP addresses DNS record types and their purposes (A, CNAME, MX, etc.) Configuring DNS settings and working with DNS providers","title":"DNS (Domain Name System)"},{"location":"level1/networking/plan/#firewalls-statelessstateful","text":"Difference between stateless and stateful firewalls Explanation of firewall rules and their impact on network traffic Practical examples of configuring and managing firewalls","title":"Firewalls (Stateless/Stateful)"},{"location":"level1/networking/plan/#cidrsubnetting","text":"Understanding CIDR (Classless Inter-Domain Routing) and its role in IP address allocation Subnetting and subnet mask calculations Practical exercises and examples of subnetting","title":"CIDR/Subnetting"},{"location":"level1/networking/plan/#httphttpsssltls","text":"Overview of HTTP (Hypertext Transfer Protocol) and HTTPS (HTTP Secure) Understanding SSL (Secure Sockets Layer) and TLS (Transport Layer Security) protocols Importance of encryption and secure communication over the web","title":"HTTP/HTTPS/SSL/TLS"},{"location":"level1/networking/plan/#forward-vs-reverse-proxy-load-balancers","text":"Difference between forward and reverse proxies Use cases and benefits of using load balancers in distributed systems Configuring and managing load balancers for efficient traffic distribution","title":"Forward vs Reverse Proxy (Load Balancers)"},{"location":"level1/networking/protocols/","text":"Protocols OSI Recap \u2705 Abstraction or Concept of the Internet Layers 1 to 7: Layer 1: Physical Layer 2: Data frames and more Layer 3: IP Layer 4: TCP/UDP, Ports Layer 5: Session layer (e.g., Linkerd) Layer 6: Presentation Layer 7: Application layer (HTTP, HTTPS, etc.) TCP/IP Model A simplified model focusing on Layers 3/4 & 7 TCP \u2705 (Layer 4) Transmission Control Protocol Controls transmission, unlike UDP which is like a firehose Connection-oriented Requires a handshake 20-byte header segment Stateful Ideal for reliable communications, SSH, database connections, web communications, etc. TCP Connection Connection represents a session An agreement between client and server Identified by four properties: SourceIP-SourcePort and DestinationIP-DestinationPort Requires a 3-way TCP handshake (SYN, SYN-ACK, ACK) Segments are sequenced, ordered, acknowledged, and retransmitted if lost Multiplexing and Demultiplexing Sender multiplexes applications into TCP connections Receiver demultiplexes TCP segments to apps based on connection pairs Connection Establishment Three-way handshake process described with an example Sending Data Data encapsulation in segments and acknowledgement process Hint: Can new segments be sent before the acknowledgement of old segments? Handling Lost Data Resending lost segments Closing Connection Four-step process: FIN, ACK, FIN, ACK Summary Layer 4 protocol Introduces the concept of connection Features retransmission, acknowledgement, and guaranteed delivery Stateful with a defined state for connections Pros & Cons of TCP Pros: Guaranteed delivery, flow control, ordered packets, secure Cons: Larger overhead, higher bandwidth usage, stateful nature, high latency for certain workloads, issues like TCP meltdown UDP \u2705 (Layer 4) User Datagram Protocol A simple protocol for sending and receiving data. No prior communication required (this can be convenient but also a security risk). Stateless - no information is stored on the host. Features an 8-byte header datagram. Use cases: video streaming, VPN, DNS, WebRTC. Multiplexing & Demultiplexing Sender multiplexes all its applications into UDP. Receiver demultiplexes UDP datagrams to each application. Source & Destination Port Example: App1 on 10.0.0.1 sends data to AppX on 10.0.0.2. Destination Port = 53. AppX responds back to App1. Source Port is necessary for return data (e.g., Source Port = 5555). UDP Pros & Cons Pros: Simplistic protocol. Small header size results in smaller datagrams. Uses less bandwidth. Stateless nature. Consumes less memory (no state stored on server/client). Cons: No acknowledgements (Acks). No guaranteed delivery. Connection-less \u2013 anyone can send data without prior knowledge. No flow control. No congestion control. Unordered packets. Security concerns \u2013 susceptibility to spoofing. Summary UDP is a Layer 4 protocol. Utilises ports to address processes. Stateless in nature. TLS (Typically Layer 5 due to its stateful nature) Transport Layer Security (TLS) Generally associated with Layer 5 due to its session management capabilities. Vanilla HTTP Client sends a GET request (without a body) to the Server. Process: Transforms into a TCP segment. Becomes an IP packet. The server listens on port 80. Server receives the segment and understands the request. The application responds with headers and the requested content (e.g., index.html). HTTPS (HTTP over TLS) Begins with establishing a connection and performing a handshake. Handshake aims to exchange symmetric keys for encryption. Client's GET request is encrypted using these keys. The server decrypts the received data using the same key. Encryption methods: Symmetric key: The same key is used for both encryption and decryption. Asymmetric keys: Different keys are used for encryption and decryption. TLS 1.2 Uses a private key to encrypt the symmetric key. The Heartbleed OpenSSL bug (2012-2014) led to scepticism about using RSA, as it lacks forward secrecy. Diffie-Hellman Key Exchange Utilises two private keys and one symmetric key. One private key for the client and one for the server. TLS 1.3 Improvements An enhanced version using the Diffie-Hellman algorithm. Mathematical basis: Server-side equation: \\((g^x \\% n)^y = g^{xy} \\% n\\) . Client-side equation: \\((g^y \\% n)^x = g^{xy} \\% n\\) . TLS 1.3 is currently the preferred standard due to its security and efficiency. HTTP/1.1 HTTP request Method (GET, POST, PUT, DELETE, PATCH, OPTIONS) Path (/, /about, etc etc) Protocol (HTTP/HTTP 1.1, HTTP 1.2, gRPC) Headers (key-value, like User-Agent, host header etc) Body (GET request doesn\u2019t have any, POST, DELETE and other methods do) example: curl -v http://google.com/about HTTP response Protocol (HTTP/1.1, HTTP/2) Code (200, 301, etc etc) Location: url content-type: text/html server: where site is hosted. HTTPS (port 444 is introduced here too) after TCP, do a TLS handshake in TLS handshake, both the client and the server will agree on the symmetric key (and symmetric encryption is used here, much faster than asymmetric encryption) same key encrypts and same key decrypts - because its same key used, key exchange is critical. so we use something called RSA or diffi hellman to exchange keys then keys are used to encrypt the GET request and headers are sent back and content using encrypted keys HTTP 1.0 New TCP connection with each request (after each request, the connection is closed to save CPU & memory) Slow Bufferring (trasnfer-encoding: chubked didnt exist) No multi-home websites (HOST header) HTTP 1.1 Persisted TCP connection: Connection is not closed immediately with each request. All use same TCP connection. Low latency & Low CPU usage HTTP request smuggling could happen. When server doesn\u2019t know where requests end or start Streaming with chunked transfer Pipelining (can run multiple requests in parallel without having to wait for response. Disabled by default and not guaranteed it will work correctly Proxying & multi-homed websites (1 IP can host 1000s of websites, the URL of the website is searched when looking for the site and not the IP) HTTP/2 Used to be called SPDY & invented by Google Compression (both headers and the body). Header compression was disabled in HTTP/1 due to an attack called Crime (hackers were able to extract key info due to the way headers were compressed) Multiplexing Sever push Secure by default (coz of protocol ossification) Protocol negotiation during TLS HTTP over QUIC (HTTP/3) Replaces TCP with QUIC (UDP with congestion control) All HTTP/2 features Without HOL (head of line blocking?) Websockets Bidirectional communication designed for the web Websockets are built on top of HTTP HTTP 1.0 (connection is not persistent, keeps closing), HTTP 1.1 (persisitent connection), then an opportunity for websockets coz connection is alive now Websocket proces Open connection Websocket handshake (just a HTTP request with a special semantic. You get a connection which now says your connection is now web sockety Because the connection is alive, we can do loads of bidirectional communication. (This couldn\u2019t be done with HTTP 1.0 coz connection is not always alive) Websockets handshake ws:// or wss:// (secure version) Client server example Req for websocket GET /chat HTTP/1.1 Host: server.exmaple.com websocket Response for ws HTTP/1.1 - 101 Switching protocol code Upgrade: websocket Some more headers like key etc etc Use cases: chatting, live feed, multiplayer gaming, showing client progress/logging , whatsApp, discord Some other useful info; Ping Pong is sent frequently to keep the websocket connection alive. Pros & Cons Pros: Full duplex (no polling) - left and right - bidirectional comms HTTP compatible (more friendly) (note: if you used TCP, you would have to build your own port) Firewall friendly (standard) Cons Proxying is tricky (need to terminate websocket connection & read message & establish another websocket connection L7 LB challenging (due to timeouts) (L4 LB can be done but kind of consuming on the backend) Stateful (knowledge of both client and server), difficult to horizontally scale Do you have to use websockets? No Rule of thumb - do you absolutely need bidirectional comms Long polling Plus websockets have extra headers and stuff to be managed. Just extra stuff Server Sent Events Note: Clients are connected to 1 server where messages are received. No one is connected to each other directly except through server where messages are broadcasted HTTP/2 (improved version of HTTP/1) HTTP 1.1 recap make req, you get 200, OK and get HTML docs (index.html, main.css, main.js, img1.jpg) Browser can can use 6 concurrent request using 6 SEPARATE tcp connections (not sure why 6 but chrome picked it?) HTTP/2 Send all concurrent requests using same TCP connection. Every request is tagged with a unique stream ID Client uses odd stream ID numbers to send (thats how you know its the client sending and the Server sends responses using an even stream ID number to send back responses HTTP/2 Push legacy and not commonly used Push mechanism is not scaleable Pros Multiplexing over single connection (save resources - 1 connection instead of 6 resources like in HTTP/1.1) Compress both headers and data/body Server Push secure by default (because of protocol ossification) Protocol negotiataion during TLS Cons TCP head of line blocking (when you send a request like 5/6 streams and you send in order, the TCP will label the bytes/segments in order. For e.g. if you send 10 requests and lets say the first req is dropped. If the server recieves req 2-10, the server will not process any of them. Server push never picked up High CPU usage Main reason why HTTP/2 was designed was because the client sends a lot of requests in the same connection. That\u2019s the only reason why it was built. Because we want to send multiple requests concurrently. HTTP/3 - HTTP over QUIC multiplexed streams HTTP 1.1 recap using 1 connection per request (slow) How HTTP/2 works? Recap multiple streams for requests in 1 connection HTTP/2 cons recap TCP head of line blocking TCP segments must be delivered in order but streams dont have to blocking requests How HTTP/3 & QUIC saves the day HTTP/3 is built on top of QUIC (and QUIC is built on UDP) like HTTP/2, QUIC has streams but QUIC uses UDP instead HTTP/3 streams - (the request are now datagrams instead of TCP segments) QUIC manages the streams and sequences (its kind of like a \u201cTCP connection\u201d for each stream Pros Merges connection setup & TLS in one handshake (secure by default), I want QUIC to be secure by default TLS 1.3 and the handshake in one handshake. So one round trip give you a connection setup, the security and the encryption Congestion control at stream level Connection migration (coz UDP is stateless, we have to send the connectionID with every single packet we send. The connectionID is used to uniquely identify the connection. So multiple connections can be running. And coz we send the connectionID, every datagram is labelled with the connectionID, every datagram is labelled with the streamID Interestingly, the connectionID is not encrypted and is in plaintext. There\u2019s a paper talking about connection hijacking Why not HTTP/2 over QUIC? Header compression algorithm Cons Takes a lot of CPU (due to parsing logic), more than HTTP/2 UDP could be blocked (sometimes proxies like in enterprise systems block UDP IP fragmentation issues (coz QUIC uses UDP and UDP has no order etc, segments can be spoofed etc etc) Interesting; QUIC is actually the reverse of HTTP/2 - odd streams are server while even streams are the client QUIC is a connection based system. Yes it\u2019s built on top of UDP, they built this virrtual connection concept at the endpoint gRPC (Google remote procedural call) built on top of HTTP takes advantage of HTTP/2 streams feattures to give various features such as bidirectional. All in 1 protocol?? Client server comms SOAP, REST< GraphQL SSE, WebSockets raw TCP Problerm with client librarries each library has its own language patching issues etc etc why gRPC was invvented? one library for popular langs Protocol: HTTP/2 (hidden implementation) Message format: protocol buffers as format gRPC modes unary RPC (basically req response) server streaming RPC client streaming RPC bidirectional streaming RPC Pros Fast & compact One client library Progress feedback (upload) Cancel req (H2) H2/protobuf Cons Schema Thick client proxies are tricky to implement in gRPX but its been done can do L7 LB/reverse proxy using nginx with gRPC but its tricky. you can build a gRPC web proxy, where you can point your web app to the gRPC proxy and proxy will convert them into an actual gRPC course (like a sidecar pattern) error handling (need to build your own, no native one) no native browser support (gRPC is entrenched to HTTP/2, uses low level calls to HTTP/2 streams those APIs dont exist in the browser because hte browser doesn\u2019t expose them timeouts (pub/sub) Can I write my own protocol? yes, you can. Spotify did something similar. Look at \u201cThe Story of Whhy we migrate to gRPC and How We Go about it\u201d talk in kubecon Europe 2019 (Matthias Gruter, Spotify) Their protocol was called Herms (written in 2012, based on zeroMQ, JSON or Protobuf payload, not a PRC framework Spotify moved to gRPC eventually not because of limitation of Hermes but because they are isoalated. gRPC is most popular. gRPC is most popular in microservices now. if 2 services want to talk to each other, the de-facto is gRPC. Many ways to HTTPS HTTP Communication basics Establish connection with backend establish encryption (HTTPS right. TLS!) Send data Close connection (when done) HTTPS over TCP with TLS1.2 TCP connect (syn syn/ack ack) >> establish TCP connection handshake (client hello, server hello, client fin, server fin) >> client and server do handshake in order to agree on symmetric key. They both do key exchange algorithm and both have the symmetric key for encrypting and decrypting data send (GET /, 200 OK) HTTPS over TCP with TLS 1.3 same as HTTPS over TCP with TLS 1.2 but with one less roundtrip (on the key exchange part) better to use TLS 1.3 unless you have a backward compatibility issue HTTPS over QUIC aka HTTP/3 the 3 way handshake and the TLS handshake happens in the same router. It\u2019s a matter of carrying the same packets in the same time so we effectively combined TLS 1.3 with the handshake of QUIC all of it is built on UDP so all of the requests are a bunch of UDP segments - datagrams going back and forth HTTPS over TFO with TLS 1.3 (TCP Fast Open, not really secure) HTTP over TCP with TLS1.3 0RTT (0 round trip) if a prior TLS connection existed between client and server, a pre shared key could have been shared to the client 0 round trip so less waiting time so less latency HTTPS over QUIC 0RTT if pre shared key was shared before and client knows about it, it can effectively send the QUIC handshake This is the fastest you can go. You can set the connection req for QUIC, in the same breath, send TLS - use pre-shared key to encrypt and also send GET req in the same breath. if this can be done, you can get extreme response time! very very fast so far only Cloudflare can manage to do this effectively in their environment (read article by Cloudflare called \u201cEven faster connection with QUIC 0-RTT resumption\u201d This is quite difficult to do but really fast.","title":"Protocols"},{"location":"level1/networking/protocols/#protocols","text":"","title":"Protocols"},{"location":"level1/networking/protocols/#osi-recap","text":"Abstraction or Concept of the Internet Layers 1 to 7: Layer 1: Physical Layer 2: Data frames and more Layer 3: IP Layer 4: TCP/UDP, Ports Layer 5: Session layer (e.g., Linkerd) Layer 6: Presentation Layer 7: Application layer (HTTP, HTTPS, etc.) TCP/IP Model A simplified model focusing on Layers 3/4 & 7","title":"OSI Recap \u2705"},{"location":"level1/networking/protocols/#tcp-layer-4","text":"Transmission Control Protocol Controls transmission, unlike UDP which is like a firehose Connection-oriented Requires a handshake 20-byte header segment Stateful Ideal for reliable communications, SSH, database connections, web communications, etc. TCP Connection Connection represents a session An agreement between client and server Identified by four properties: SourceIP-SourcePort and DestinationIP-DestinationPort Requires a 3-way TCP handshake (SYN, SYN-ACK, ACK) Segments are sequenced, ordered, acknowledged, and retransmitted if lost Multiplexing and Demultiplexing Sender multiplexes applications into TCP connections Receiver demultiplexes TCP segments to apps based on connection pairs Connection Establishment Three-way handshake process described with an example Sending Data Data encapsulation in segments and acknowledgement process Hint: Can new segments be sent before the acknowledgement of old segments? Handling Lost Data Resending lost segments Closing Connection Four-step process: FIN, ACK, FIN, ACK Summary Layer 4 protocol Introduces the concept of connection Features retransmission, acknowledgement, and guaranteed delivery Stateful with a defined state for connections Pros & Cons of TCP Pros: Guaranteed delivery, flow control, ordered packets, secure Cons: Larger overhead, higher bandwidth usage, stateful nature, high latency for certain workloads, issues like TCP meltdown","title":"TCP \u2705 (Layer 4)"},{"location":"level1/networking/protocols/#udp-layer-4","text":"User Datagram Protocol A simple protocol for sending and receiving data. No prior communication required (this can be convenient but also a security risk). Stateless - no information is stored on the host. Features an 8-byte header datagram. Use cases: video streaming, VPN, DNS, WebRTC. Multiplexing & Demultiplexing Sender multiplexes all its applications into UDP. Receiver demultiplexes UDP datagrams to each application. Source & Destination Port Example: App1 on 10.0.0.1 sends data to AppX on 10.0.0.2. Destination Port = 53. AppX responds back to App1. Source Port is necessary for return data (e.g., Source Port = 5555). UDP Pros & Cons Pros: Simplistic protocol. Small header size results in smaller datagrams. Uses less bandwidth. Stateless nature. Consumes less memory (no state stored on server/client). Cons: No acknowledgements (Acks). No guaranteed delivery. Connection-less \u2013 anyone can send data without prior knowledge. No flow control. No congestion control. Unordered packets. Security concerns \u2013 susceptibility to spoofing. Summary UDP is a Layer 4 protocol. Utilises ports to address processes. Stateless in nature.","title":"UDP \u2705 (Layer 4)"},{"location":"level1/networking/protocols/#tls-typically-layer-5-due-to-its-stateful-nature","text":"Transport Layer Security (TLS) Generally associated with Layer 5 due to its session management capabilities. Vanilla HTTP Client sends a GET request (without a body) to the Server. Process: Transforms into a TCP segment. Becomes an IP packet. The server listens on port 80. Server receives the segment and understands the request. The application responds with headers and the requested content (e.g., index.html). HTTPS (HTTP over TLS) Begins with establishing a connection and performing a handshake. Handshake aims to exchange symmetric keys for encryption. Client's GET request is encrypted using these keys. The server decrypts the received data using the same key. Encryption methods: Symmetric key: The same key is used for both encryption and decryption. Asymmetric keys: Different keys are used for encryption and decryption. TLS 1.2 Uses a private key to encrypt the symmetric key. The Heartbleed OpenSSL bug (2012-2014) led to scepticism about using RSA, as it lacks forward secrecy. Diffie-Hellman Key Exchange Utilises two private keys and one symmetric key. One private key for the client and one for the server. TLS 1.3 Improvements An enhanced version using the Diffie-Hellman algorithm. Mathematical basis: Server-side equation: \\((g^x \\% n)^y = g^{xy} \\% n\\) . Client-side equation: \\((g^y \\% n)^x = g^{xy} \\% n\\) . TLS 1.3 is currently the preferred standard due to its security and efficiency.","title":"TLS (Typically Layer 5 due to its stateful nature)"},{"location":"level1/networking/protocols/#http11","text":"HTTP request Method (GET, POST, PUT, DELETE, PATCH, OPTIONS) Path (/, /about, etc etc) Protocol (HTTP/HTTP 1.1, HTTP 1.2, gRPC) Headers (key-value, like User-Agent, host header etc) Body (GET request doesn\u2019t have any, POST, DELETE and other methods do) example: curl -v http://google.com/about HTTP response Protocol (HTTP/1.1, HTTP/2) Code (200, 301, etc etc) Location: url content-type: text/html server: where site is hosted. HTTPS (port 444 is introduced here too) after TCP, do a TLS handshake in TLS handshake, both the client and the server will agree on the symmetric key (and symmetric encryption is used here, much faster than asymmetric encryption) same key encrypts and same key decrypts - because its same key used, key exchange is critical. so we use something called RSA or diffi hellman to exchange keys then keys are used to encrypt the GET request and headers are sent back and content using encrypted keys HTTP 1.0 New TCP connection with each request (after each request, the connection is closed to save CPU & memory) Slow Bufferring (trasnfer-encoding: chubked didnt exist) No multi-home websites (HOST header) HTTP 1.1 Persisted TCP connection: Connection is not closed immediately with each request. All use same TCP connection. Low latency & Low CPU usage HTTP request smuggling could happen. When server doesn\u2019t know where requests end or start Streaming with chunked transfer Pipelining (can run multiple requests in parallel without having to wait for response. Disabled by default and not guaranteed it will work correctly Proxying & multi-homed websites (1 IP can host 1000s of websites, the URL of the website is searched when looking for the site and not the IP) HTTP/2 Used to be called SPDY & invented by Google Compression (both headers and the body). Header compression was disabled in HTTP/1 due to an attack called Crime (hackers were able to extract key info due to the way headers were compressed) Multiplexing Sever push Secure by default (coz of protocol ossification) Protocol negotiation during TLS HTTP over QUIC (HTTP/3) Replaces TCP with QUIC (UDP with congestion control) All HTTP/2 features Without HOL (head of line blocking?)","title":"HTTP/1.1"},{"location":"level1/networking/protocols/#websockets","text":"Bidirectional communication designed for the web Websockets are built on top of HTTP HTTP 1.0 (connection is not persistent, keeps closing), HTTP 1.1 (persisitent connection), then an opportunity for websockets coz connection is alive now Websocket proces Open connection Websocket handshake (just a HTTP request with a special semantic. You get a connection which now says your connection is now web sockety Because the connection is alive, we can do loads of bidirectional communication. (This couldn\u2019t be done with HTTP 1.0 coz connection is not always alive) Websockets handshake ws:// or wss:// (secure version) Client server example Req for websocket GET /chat HTTP/1.1 Host: server.exmaple.com websocket Response for ws HTTP/1.1 - 101 Switching protocol code Upgrade: websocket Some more headers like key etc etc Use cases: chatting, live feed, multiplayer gaming, showing client progress/logging , whatsApp, discord Some other useful info; Ping Pong is sent frequently to keep the websocket connection alive. Pros & Cons Pros: Full duplex (no polling) - left and right - bidirectional comms HTTP compatible (more friendly) (note: if you used TCP, you would have to build your own port) Firewall friendly (standard) Cons Proxying is tricky (need to terminate websocket connection & read message & establish another websocket connection L7 LB challenging (due to timeouts) (L4 LB can be done but kind of consuming on the backend) Stateful (knowledge of both client and server), difficult to horizontally scale Do you have to use websockets? No Rule of thumb - do you absolutely need bidirectional comms Long polling Plus websockets have extra headers and stuff to be managed. Just extra stuff Server Sent Events Note: Clients are connected to 1 server where messages are received. No one is connected to each other directly except through server where messages are broadcasted","title":"Websockets"},{"location":"level1/networking/protocols/#http2-improved-version-of-http1","text":"HTTP 1.1 recap make req, you get 200, OK and get HTML docs (index.html, main.css, main.js, img1.jpg) Browser can can use 6 concurrent request using 6 SEPARATE tcp connections (not sure why 6 but chrome picked it?) HTTP/2 Send all concurrent requests using same TCP connection. Every request is tagged with a unique stream ID Client uses odd stream ID numbers to send (thats how you know its the client sending and the Server sends responses using an even stream ID number to send back responses HTTP/2 Push legacy and not commonly used Push mechanism is not scaleable Pros Multiplexing over single connection (save resources - 1 connection instead of 6 resources like in HTTP/1.1) Compress both headers and data/body Server Push secure by default (because of protocol ossification) Protocol negotiataion during TLS Cons TCP head of line blocking (when you send a request like 5/6 streams and you send in order, the TCP will label the bytes/segments in order. For e.g. if you send 10 requests and lets say the first req is dropped. If the server recieves req 2-10, the server will not process any of them. Server push never picked up High CPU usage Main reason why HTTP/2 was designed was because the client sends a lot of requests in the same connection. That\u2019s the only reason why it was built. Because we want to send multiple requests concurrently.","title":"HTTP/2 (improved version of HTTP/1)"},{"location":"level1/networking/protocols/#http3-http-over-quic-multiplexed-streams","text":"HTTP 1.1 recap using 1 connection per request (slow) How HTTP/2 works? Recap multiple streams for requests in 1 connection HTTP/2 cons recap TCP head of line blocking TCP segments must be delivered in order but streams dont have to blocking requests How HTTP/3 & QUIC saves the day HTTP/3 is built on top of QUIC (and QUIC is built on UDP) like HTTP/2, QUIC has streams but QUIC uses UDP instead HTTP/3 streams - (the request are now datagrams instead of TCP segments) QUIC manages the streams and sequences (its kind of like a \u201cTCP connection\u201d for each stream Pros Merges connection setup & TLS in one handshake (secure by default), I want QUIC to be secure by default TLS 1.3 and the handshake in one handshake. So one round trip give you a connection setup, the security and the encryption Congestion control at stream level Connection migration (coz UDP is stateless, we have to send the connectionID with every single packet we send. The connectionID is used to uniquely identify the connection. So multiple connections can be running. And coz we send the connectionID, every datagram is labelled with the connectionID, every datagram is labelled with the streamID Interestingly, the connectionID is not encrypted and is in plaintext. There\u2019s a paper talking about connection hijacking Why not HTTP/2 over QUIC? Header compression algorithm Cons Takes a lot of CPU (due to parsing logic), more than HTTP/2 UDP could be blocked (sometimes proxies like in enterprise systems block UDP IP fragmentation issues (coz QUIC uses UDP and UDP has no order etc, segments can be spoofed etc etc) Interesting; QUIC is actually the reverse of HTTP/2 - odd streams are server while even streams are the client QUIC is a connection based system. Yes it\u2019s built on top of UDP, they built this virrtual connection concept at the endpoint","title":"HTTP/3 - HTTP over QUIC multiplexed streams"},{"location":"level1/networking/protocols/#grpc-google-remote-procedural-call","text":"built on top of HTTP takes advantage of HTTP/2 streams feattures to give various features such as bidirectional. All in 1 protocol?? Client server comms SOAP, REST< GraphQL SSE, WebSockets raw TCP Problerm with client librarries each library has its own language patching issues etc etc why gRPC was invvented? one library for popular langs Protocol: HTTP/2 (hidden implementation) Message format: protocol buffers as format gRPC modes unary RPC (basically req response) server streaming RPC client streaming RPC bidirectional streaming RPC Pros Fast & compact One client library Progress feedback (upload) Cancel req (H2) H2/protobuf Cons Schema Thick client proxies are tricky to implement in gRPX but its been done can do L7 LB/reverse proxy using nginx with gRPC but its tricky. you can build a gRPC web proxy, where you can point your web app to the gRPC proxy and proxy will convert them into an actual gRPC course (like a sidecar pattern) error handling (need to build your own, no native one) no native browser support (gRPC is entrenched to HTTP/2, uses low level calls to HTTP/2 streams those APIs dont exist in the browser because hte browser doesn\u2019t expose them timeouts (pub/sub) Can I write my own protocol? yes, you can. Spotify did something similar. Look at \u201cThe Story of Whhy we migrate to gRPC and How We Go about it\u201d talk in kubecon Europe 2019 (Matthias Gruter, Spotify) Their protocol was called Herms (written in 2012, based on zeroMQ, JSON or Protobuf payload, not a PRC framework Spotify moved to gRPC eventually not because of limitation of Hermes but because they are isoalated. gRPC is most popular. gRPC is most popular in microservices now. if 2 services want to talk to each other, the de-facto is gRPC.","title":"gRPC (Google remote procedural call)"},{"location":"level1/networking/protocols/#many-ways-to-https","text":"HTTP Communication basics Establish connection with backend establish encryption (HTTPS right. TLS!) Send data Close connection (when done) HTTPS over TCP with TLS1.2 TCP connect (syn syn/ack ack) >> establish TCP connection handshake (client hello, server hello, client fin, server fin) >> client and server do handshake in order to agree on symmetric key. They both do key exchange algorithm and both have the symmetric key for encrypting and decrypting data send (GET /, 200 OK) HTTPS over TCP with TLS 1.3 same as HTTPS over TCP with TLS 1.2 but with one less roundtrip (on the key exchange part) better to use TLS 1.3 unless you have a backward compatibility issue HTTPS over QUIC aka HTTP/3 the 3 way handshake and the TLS handshake happens in the same router. It\u2019s a matter of carrying the same packets in the same time so we effectively combined TLS 1.3 with the handshake of QUIC all of it is built on UDP so all of the requests are a bunch of UDP segments - datagrams going back and forth HTTPS over TFO with TLS 1.3 (TCP Fast Open, not really secure) HTTP over TCP with TLS1.3 0RTT (0 round trip) if a prior TLS connection existed between client and server, a pre shared key could have been shared to the client 0 round trip so less waiting time so less latency HTTPS over QUIC 0RTT if pre shared key was shared before and client knows about it, it can effectively send the QUIC handshake This is the fastest you can go. You can set the connection req for QUIC, in the same breath, send TLS - use pre-shared key to encrypt and also send GET req in the same breath. if this can be done, you can get extreme response time! very very fast so far only Cloudflare can manage to do this effectively in their environment (read article by Cloudflare called \u201cEven faster connection with QUIC 0-RTT resumption\u201d This is quite difficult to do but really fast.","title":"Many ways to HTTPS"},{"location":"level1/networking/proxies/","text":"Proxy & Reverse Proxy Proxy a server that makes requests on your behalf (client knows the server but server doesn\u2019t know client) your TCP is connection is being established with a proxy before going to the destination establsh a TCP connection between you and the content client of Layer 7 will go to google.com. You send a GET / request and the request will go to google.com `use case caching, anonymity, logging, block sites, microservices fiddler (mitmproxy, a common proxy to monitor your requests locally) for a proxy from a layer 4 (TCP) perspective, the proxy is the final destination from a layer 7 (HTTP) perspective, the backend server is the final destination Reverse proxy (opposite of proxy) RP= reverse proxy client does not know the final destination (the actual server the site is hosted where the request lands so the client only knows that google.com is the final proxy (the frontend/edge server) but google.com routes requests to the google-server (backend servers) behind the scenes load balancing was born from this this reverse proxy server can load balance between the backend servers and use things like round robin and various other LB methods the RP can even take requests to different servers based on the path youre going so each server can be separated for different tools (one server for POST, another server for GET etc) so a load balancer is reverse proxy but not every reverse proxy is a load balancer an PR is just that it makes request to something on the backend you don\u2019t know about for a reverse proxy from a lyer 4 and layer 7 perspective, the reverse proxy is the final destination Use cases caching/CDN (cdn is basically a glorified reverse proxy) load balancing (balance your request to multiple servers on the backend) ingress canary deployment micro services can proxy and reverse proxy be used at the same time? yes but you won\u2019t know Can i use proxy instead of VPN for anonymity? no, its not a good idea because some proxies terminate aliases and look at your content VPN operate at the IP level. so any IP packet, they encrypt Proxy operates at L4 and above so it needs to know about ports and protocol Is proxy just for HTTP traffic? not really there are many types of proxies but HTTP is the most popular there is a mode when you use HTTPS proxy called tunnel mode: where the client can ask the proxy to open a connection for it tunnel mode: research this!","title":"Proxies"},{"location":"level1/networking/proxies/#proxy-reverse-proxy","text":"Proxy a server that makes requests on your behalf (client knows the server but server doesn\u2019t know client) your TCP is connection is being established with a proxy before going to the destination establsh a TCP connection between you and the content client of Layer 7 will go to google.com. You send a GET / request and the request will go to google.com `use case caching, anonymity, logging, block sites, microservices fiddler (mitmproxy, a common proxy to monitor your requests locally) for a proxy from a layer 4 (TCP) perspective, the proxy is the final destination from a layer 7 (HTTP) perspective, the backend server is the final destination Reverse proxy (opposite of proxy) RP= reverse proxy client does not know the final destination (the actual server the site is hosted where the request lands so the client only knows that google.com is the final proxy (the frontend/edge server) but google.com routes requests to the google-server (backend servers) behind the scenes load balancing was born from this this reverse proxy server can load balance between the backend servers and use things like round robin and various other LB methods the RP can even take requests to different servers based on the path youre going so each server can be separated for different tools (one server for POST, another server for GET etc) so a load balancer is reverse proxy but not every reverse proxy is a load balancer an PR is just that it makes request to something on the backend you don\u2019t know about for a reverse proxy from a lyer 4 and layer 7 perspective, the reverse proxy is the final destination Use cases caching/CDN (cdn is basically a glorified reverse proxy) load balancing (balance your request to multiple servers on the backend) ingress canary deployment micro services can proxy and reverse proxy be used at the same time? yes but you won\u2019t know Can i use proxy instead of VPN for anonymity? no, its not a good idea because some proxies terminate aliases and look at your content VPN operate at the IP level. so any IP packet, they encrypt Proxy operates at L4 and above so it needs to know about ports and protocol Is proxy just for HTTP traffic? not really there are many types of proxies but HTTP is the most popular there is a mode when you use HTTPS proxy called tunnel mode: where the client can ask the proxy to open a connection for it tunnel mode: research this!","title":"Proxy &amp; Reverse Proxy"},{"location":"level2/cicd/intro/","text":"CI/CD Introduction","title":"Intro to CICD"},{"location":"level2/cicd/intro/#cicd-introduction","text":"","title":"CI/CD Introduction"},{"location":"level2/cicd/plan/","text":"Hands-On CI/CD Syllabus for DevOps Beginners \ud83d\udd04 (Focusing on GitHub Actions) Module 1: Introduction to CI/CD and DevOps Mindset DevOps and CI/CD: An Overview Understanding the role of CI/CD in the DevOps lifecycle. Introduction to GitHub Actions as a CI/CD tool. Setting Up for Success Configuring a development environment for CI/CD practice. Introduction to GitHub repository setup. Module 2: Getting Started with GitHub Actions Introduction to GitHub Actions Understanding workflows, events, jobs, steps, and actions. Creating your first GitHub Actions workflow. Automating Basic Tasks Writing custom scripts for automation. Triggering workflows on code pushes and pull requests. Module 3: Continuous Integration with GitHub Actions Building a CI Pipeline Automating code integration processes. Implementing build and test automation in workflows. Integrating with Testing Frameworks Setting up unit and integration tests. Managing test results and feedback. Module 4: Continuous Delivery and Deployment Implementing Continuous Delivery Understanding deployment concepts in GitHub Actions. Building deployment pipelines. Deployment Strategies and Best Practices Exploring different deployment strategies (e.g., Blue/Green). Managing environment variables and secrets. Module 5: Infrastructure as Code and GitHub Actions IaC with GitHub Actions Integrating Terraform or similar IaC tools with GitHub Actions. Automating infrastructure provisioning. Best Practices for IaC in CI/CD Version control for infrastructure code. Handling infrastructure changes in workflows. Module 6: Advanced GitHub Actions Features Advanced Workflows Using matrix builds for multiple environments. Managing dependencies and caching. Security and Compliance in CI/CD Implementing security scanning and compliance checks. Managing sensitive data with GitHub secrets. Module 7: Monitoring and Feedback in CI/CD Pipelines Monitoring and Observability Integrating monitoring tools into CI/CD pipelines. Setting up alerts and notifications. Feedback Loops and Continuous Improvement Analyzing build logs and metrics. Iterating on feedback for process improvement. Module 8: Real-world Applications and Case Studies Case Studies Exploring real-world examples of CI/CD implementations. CI/CD Project Building a complete CI/CD pipeline with GitHub Actions. Deploying a web application or service. Module 9: Staying Current and Community Engagement Future Trends in CI/CD Exploring emerging trends and tools in CI/CD. Community and Continuous Learning Engaging with the GitHub community. Resources for ongoing learning and development in CI/CD.","title":"Syllabus"},{"location":"level2/cicd/plan/#hands-on-cicd-syllabus-for-devops-beginners-focusing-on-github-actions","text":"","title":"Hands-On CI/CD Syllabus for DevOps Beginners \ud83d\udd04 (Focusing on GitHub Actions)"},{"location":"level2/cicd/plan/#module-1-introduction-to-cicd-and-devops-mindset","text":"DevOps and CI/CD: An Overview Understanding the role of CI/CD in the DevOps lifecycle. Introduction to GitHub Actions as a CI/CD tool. Setting Up for Success Configuring a development environment for CI/CD practice. Introduction to GitHub repository setup.","title":"Module 1: Introduction to CI/CD and DevOps Mindset"},{"location":"level2/cicd/plan/#module-2-getting-started-with-github-actions","text":"Introduction to GitHub Actions Understanding workflows, events, jobs, steps, and actions. Creating your first GitHub Actions workflow. Automating Basic Tasks Writing custom scripts for automation. Triggering workflows on code pushes and pull requests.","title":"Module 2: Getting Started with GitHub Actions"},{"location":"level2/cicd/plan/#module-3-continuous-integration-with-github-actions","text":"Building a CI Pipeline Automating code integration processes. Implementing build and test automation in workflows. Integrating with Testing Frameworks Setting up unit and integration tests. Managing test results and feedback.","title":"Module 3: Continuous Integration with GitHub Actions"},{"location":"level2/cicd/plan/#module-4-continuous-delivery-and-deployment","text":"Implementing Continuous Delivery Understanding deployment concepts in GitHub Actions. Building deployment pipelines. Deployment Strategies and Best Practices Exploring different deployment strategies (e.g., Blue/Green). Managing environment variables and secrets.","title":"Module 4: Continuous Delivery and Deployment"},{"location":"level2/cicd/plan/#module-5-infrastructure-as-code-and-github-actions","text":"IaC with GitHub Actions Integrating Terraform or similar IaC tools with GitHub Actions. Automating infrastructure provisioning. Best Practices for IaC in CI/CD Version control for infrastructure code. Handling infrastructure changes in workflows.","title":"Module 5: Infrastructure as Code and GitHub Actions"},{"location":"level2/cicd/plan/#module-6-advanced-github-actions-features","text":"Advanced Workflows Using matrix builds for multiple environments. Managing dependencies and caching. Security and Compliance in CI/CD Implementing security scanning and compliance checks. Managing sensitive data with GitHub secrets.","title":"Module 6: Advanced GitHub Actions Features"},{"location":"level2/cicd/plan/#module-7-monitoring-and-feedback-in-cicd-pipelines","text":"Monitoring and Observability Integrating monitoring tools into CI/CD pipelines. Setting up alerts and notifications. Feedback Loops and Continuous Improvement Analyzing build logs and metrics. Iterating on feedback for process improvement.","title":"Module 7: Monitoring and Feedback in CI/CD Pipelines"},{"location":"level2/cicd/plan/#module-8-real-world-applications-and-case-studies","text":"Case Studies Exploring real-world examples of CI/CD implementations. CI/CD Project Building a complete CI/CD pipeline with GitHub Actions. Deploying a web application or service.","title":"Module 8: Real-world Applications and Case Studies"},{"location":"level2/cicd/plan/#module-9-staying-current-and-community-engagement","text":"Future Trends in CI/CD Exploring emerging trends and tools in CI/CD. Community and Continuous Learning Engaging with the GitHub community. Resources for ongoing learning and development in CI/CD.","title":"Module 9: Staying Current and Community Engagement"},{"location":"level2/docker/intro/","text":"Docker Introduction","title":"Intro"},{"location":"level2/docker/intro/#docker-introduction","text":"","title":"Docker Introduction"},{"location":"level2/docker/plan/","text":"Docker Syllabus for DevOps Beginners \ud83d\udc33 Module 1: Introduction to Docker and Containerization Understanding Containerization What is Containerization? Docker vs. Virtual Machines. Introduction to Docker History and Importance of Docker in DevOps. Key Concepts: Images, Containers, Docker Hub. Module 2: Docker Basics Installing Docker Installation process for various operating systems. Docker CLI Basics Commonly used Docker commands: run , ps , stop , rm . Understanding Docker images and containers. Docker Images Pulling images from Docker Hub. Understanding and managing image layers. Module 3: Docker Containers Running Containers Creating and running Docker containers. Container isolation and resource sharing. Container Management Inspecting, stopping, and removing containers. Port mapping and volume mounting. Docker Networking Basic networking concepts in Docker. Connecting containers and using Docker networks. Module 4: Building Custom Docker Images Dockerfile Basics Writing your first Dockerfile. Docker build process and layers. Building and Managing Images Building images from Dockerfiles. Managing and sharing images through Docker Hub. Best Practices for Writing Dockerfiles Optimizing Docker image builds. Security best practices in Docker images. Module 5: Docker Compose and Multi-Container Applications Introduction to Docker Compose Understanding the role of Docker Compose. Writing docker-compose.yml files. Orchestrating Multi-container Applications Setting up and managing multi-container environments. Networking and volume management with Docker Compose. Module 6: Advanced Topics and Best Practices Docker in Production Deploying Docker in a production environment. Scaling and updating containerized applications. Docker Security Security considerations and best practices. Securing Docker images and runtime. Monitoring and Logging Monitoring container performance. Logging best practices in Docker environments. Module 7: Future Trends and Community Resources Keeping Up with Docker Following Docker updates and new features. Community and Resources Engaging with the Docker community. Exploring further learning resources.","title":"Syllabus"},{"location":"level2/docker/plan/#docker-syllabus-for-devops-beginners","text":"","title":"Docker Syllabus for DevOps Beginners \ud83d\udc33"},{"location":"level2/docker/plan/#module-1-introduction-to-docker-and-containerization","text":"Understanding Containerization What is Containerization? Docker vs. Virtual Machines. Introduction to Docker History and Importance of Docker in DevOps. Key Concepts: Images, Containers, Docker Hub.","title":"Module 1: Introduction to Docker and Containerization"},{"location":"level2/docker/plan/#module-2-docker-basics","text":"Installing Docker Installation process for various operating systems. Docker CLI Basics Commonly used Docker commands: run , ps , stop , rm . Understanding Docker images and containers. Docker Images Pulling images from Docker Hub. Understanding and managing image layers.","title":"Module 2: Docker Basics"},{"location":"level2/docker/plan/#module-3-docker-containers","text":"Running Containers Creating and running Docker containers. Container isolation and resource sharing. Container Management Inspecting, stopping, and removing containers. Port mapping and volume mounting. Docker Networking Basic networking concepts in Docker. Connecting containers and using Docker networks.","title":"Module 3: Docker Containers"},{"location":"level2/docker/plan/#module-4-building-custom-docker-images","text":"Dockerfile Basics Writing your first Dockerfile. Docker build process and layers. Building and Managing Images Building images from Dockerfiles. Managing and sharing images through Docker Hub. Best Practices for Writing Dockerfiles Optimizing Docker image builds. Security best practices in Docker images.","title":"Module 4: Building Custom Docker Images"},{"location":"level2/docker/plan/#module-5-docker-compose-and-multi-container-applications","text":"Introduction to Docker Compose Understanding the role of Docker Compose. Writing docker-compose.yml files. Orchestrating Multi-container Applications Setting up and managing multi-container environments. Networking and volume management with Docker Compose.","title":"Module 5: Docker Compose and Multi-Container Applications"},{"location":"level2/docker/plan/#module-6-advanced-topics-and-best-practices","text":"Docker in Production Deploying Docker in a production environment. Scaling and updating containerized applications. Docker Security Security considerations and best practices. Securing Docker images and runtime. Monitoring and Logging Monitoring container performance. Logging best practices in Docker environments.","title":"Module 6: Advanced Topics and Best Practices"},{"location":"level2/docker/plan/#module-7-future-trends-and-community-resources","text":"Keeping Up with Docker Following Docker updates and new features. Community and Resources Engaging with the Docker community. Exploring further learning resources.","title":"Module 7: Future Trends and Community Resources"},{"location":"level2/k8s/intro/","text":"K8s Introduction","title":"Basics"},{"location":"level2/k8s/intro/#k8s-introduction","text":"","title":"K8s Introduction"},{"location":"level2/k8s/plan/","text":"Hands-On Kubernetes Syllabus for DevOps Beginners \ud83d\ude80 Module 1: Introduction to Kubernetes and Container Orchestration What is Kubernetes? Understanding Kubernetes and its role in modern DevOps. Overview of container orchestration. Kubernetes Architecture Components of a Kubernetes cluster. Master vs. Worker Nodes. Module 2: Setting Up a Kubernetes Environment Local Development Setup Installing and configuring Minikube or kind (Kubernetes in Docker). Introduction to kubectl, the Kubernetes command-line tool. Exploring Kubernetes Dashboard Installing and navigating the Kubernetes Dashboard. Visualizing cluster activities. Module 3: Kubernetes Core Concepts Pods and Containers Creating and managing pods. Understanding the lifecycle of pods and containers. Services and Networking Exposing applications with Services. Basics of Kubernetes networking. Module 4: Deep Dive into Kubernetes Workloads Deployments and ReplicaSets Managing applications with Deployments. Understanding ReplicaSets for scaling and self-healing. StatefulSets and DaemonSets Use cases for StatefulSets and DaemonSets. Hands-on deployment of stateful applications. Module 5: Storage and Persistent Data Management Persistent Volumes and Persistent Volume Claims Configuring storage with PVs and PVCs. Understanding storage classes. ConfigMaps and Secrets Managing application configurations and sensitive data. Module 6: Kubernetes Networking and Ingress Networking in Kubernetes Deep dive into pod networking, CNI. Network policies for security. Ingress Controllers and Ingress Resources Managing external access to applications. Setting up an Ingress Controller and defining Ingress Resources. Module 7: Kubernetes Monitoring and Logging Implementing Monitoring Solutions Setting up Prometheus and Grafana for cluster monitoring. Understanding metrics and alerts. Logging in Kubernetes Introduction to logging with tools like Fluentd or ELK Stack. Aggregating and analyzing logs. Module 8: Advanced Kubernetes Topics Autoscaling Applications Implementing Horizontal Pod Autoscaling. Autoscaling clusters with Cluster Autoscaler. Kubernetes Security Best Practices Securing the Kubernetes cluster. Role-Based Access Control (RBAC) and security contexts. Module 9: Real-world Scenarios and Project Work Case Studies Exploring real-world Kubernetes deployments. Kubernetes Project Deploying a multi-service application on Kubernetes. Hands-on experience with end-to-end application deployment and management. Module 10: Staying Current in the Kubernetes Ecosystem Emerging Trends and Tools in Kubernetes Keeping up with the latest in Kubernetes. Community Engagement and Resources Engaging with the Kubernetes community. Resources for continuous learning and professional growth.","title":"Syllabus"},{"location":"level2/k8s/plan/#hands-on-kubernetes-syllabus-for-devops-beginners","text":"","title":"Hands-On Kubernetes Syllabus for DevOps Beginners \ud83d\ude80"},{"location":"level2/k8s/plan/#module-1-introduction-to-kubernetes-and-container-orchestration","text":"What is Kubernetes? Understanding Kubernetes and its role in modern DevOps. Overview of container orchestration. Kubernetes Architecture Components of a Kubernetes cluster. Master vs. Worker Nodes.","title":"Module 1: Introduction to Kubernetes and Container Orchestration"},{"location":"level2/k8s/plan/#module-2-setting-up-a-kubernetes-environment","text":"Local Development Setup Installing and configuring Minikube or kind (Kubernetes in Docker). Introduction to kubectl, the Kubernetes command-line tool. Exploring Kubernetes Dashboard Installing and navigating the Kubernetes Dashboard. Visualizing cluster activities.","title":"Module 2: Setting Up a Kubernetes Environment"},{"location":"level2/k8s/plan/#module-3-kubernetes-core-concepts","text":"Pods and Containers Creating and managing pods. Understanding the lifecycle of pods and containers. Services and Networking Exposing applications with Services. Basics of Kubernetes networking.","title":"Module 3: Kubernetes Core Concepts"},{"location":"level2/k8s/plan/#module-4-deep-dive-into-kubernetes-workloads","text":"Deployments and ReplicaSets Managing applications with Deployments. Understanding ReplicaSets for scaling and self-healing. StatefulSets and DaemonSets Use cases for StatefulSets and DaemonSets. Hands-on deployment of stateful applications.","title":"Module 4: Deep Dive into Kubernetes Workloads"},{"location":"level2/k8s/plan/#module-5-storage-and-persistent-data-management","text":"Persistent Volumes and Persistent Volume Claims Configuring storage with PVs and PVCs. Understanding storage classes. ConfigMaps and Secrets Managing application configurations and sensitive data.","title":"Module 5: Storage and Persistent Data Management"},{"location":"level2/k8s/plan/#module-6-kubernetes-networking-and-ingress","text":"Networking in Kubernetes Deep dive into pod networking, CNI. Network policies for security. Ingress Controllers and Ingress Resources Managing external access to applications. Setting up an Ingress Controller and defining Ingress Resources.","title":"Module 6: Kubernetes Networking and Ingress"},{"location":"level2/k8s/plan/#module-7-kubernetes-monitoring-and-logging","text":"Implementing Monitoring Solutions Setting up Prometheus and Grafana for cluster monitoring. Understanding metrics and alerts. Logging in Kubernetes Introduction to logging with tools like Fluentd or ELK Stack. Aggregating and analyzing logs.","title":"Module 7: Kubernetes Monitoring and Logging"},{"location":"level2/k8s/plan/#module-8-advanced-kubernetes-topics","text":"Autoscaling Applications Implementing Horizontal Pod Autoscaling. Autoscaling clusters with Cluster Autoscaler. Kubernetes Security Best Practices Securing the Kubernetes cluster. Role-Based Access Control (RBAC) and security contexts.","title":"Module 8: Advanced Kubernetes Topics"},{"location":"level2/k8s/plan/#module-9-real-world-scenarios-and-project-work","text":"Case Studies Exploring real-world Kubernetes deployments. Kubernetes Project Deploying a multi-service application on Kubernetes. Hands-on experience with end-to-end application deployment and management.","title":"Module 9: Real-world Scenarios and Project Work"},{"location":"level2/k8s/plan/#module-10-staying-current-in-the-kubernetes-ecosystem","text":"Emerging Trends and Tools in Kubernetes Keeping up with the latest in Kubernetes. Community Engagement and Resources Engaging with the Kubernetes community. Resources for continuous learning and professional growth.","title":"Module 10: Staying Current in the Kubernetes Ecosystem"},{"location":"level2/terraform/intro/","text":"Terraform Introduction Welcome to the Terraform Introduction chapter in the \"DevOps Pathway\" educational repository! In this chapter, we will explore the fundamentals of Terraform and its role in Infrastructure as Code (IaC). What is Terraform? Terraform is an open-source infrastructure provisioning and management tool created by HashiCorp. It allows you to define and manage your infrastructure resources using declarative configuration files. With Terraform, you can automate the creation, modification, and destruction of infrastructure components across various cloud providers and on-premises environments. Why Use Terraform? Terraform offers several benefits for infrastructure management: Infrastructure as Code : Terraform allows you to define your infrastructure as code, making it version-controlled, repeatable, and easily shareable among team members. This approach enables collaboration, transparency, and reproducibility. Multi-Cloud Provisioning : Terraform is cloud-agnostic and supports multiple cloud providers, such as AWS, Azure, and GCP, as well as other infrastructure technologies. This flexibility allows you to manage resources consistently across different cloud platforms. Resource Dependency Management : Terraform intelligently manages dependencies between resources. It automatically determines the order of resource creation, modification, and destruction based on their relationships, ensuring the integrity and consistency of your infrastructure. Plan and Preview Changes : Terraform provides a plan command that allows you to preview the changes that will be made to your infrastructure before applying them. This helps minimize human error and allows you to validate changes and ensure they align with your expectations. Scalability and Modularity : With Terraform, you can create reusable infrastructure modules that encapsulate and abstract common infrastructure patterns. This modularity promotes consistency, reduces duplication, and simplifies the management of complex infrastructure setups. Getting Started with Terraform To begin your Terraform journey, you will need to install Terraform on your local machine. Follow the official Terraform installation guide to set up Terraform on your preferred operating system. In the upcoming chapters, we will explore various Terraform concepts and dive deeper into its usage with practical examples and hands-on exercises. Stay tuned and get ready to unlock the power of infrastructure as code with Terraform!","title":"Intro"},{"location":"level2/terraform/intro/#terraform-introduction","text":"Welcome to the Terraform Introduction chapter in the \"DevOps Pathway\" educational repository! In this chapter, we will explore the fundamentals of Terraform and its role in Infrastructure as Code (IaC).","title":"Terraform Introduction"},{"location":"level2/terraform/intro/#what-is-terraform","text":"Terraform is an open-source infrastructure provisioning and management tool created by HashiCorp. It allows you to define and manage your infrastructure resources using declarative configuration files. With Terraform, you can automate the creation, modification, and destruction of infrastructure components across various cloud providers and on-premises environments.","title":"What is Terraform?"},{"location":"level2/terraform/intro/#why-use-terraform","text":"Terraform offers several benefits for infrastructure management: Infrastructure as Code : Terraform allows you to define your infrastructure as code, making it version-controlled, repeatable, and easily shareable among team members. This approach enables collaboration, transparency, and reproducibility. Multi-Cloud Provisioning : Terraform is cloud-agnostic and supports multiple cloud providers, such as AWS, Azure, and GCP, as well as other infrastructure technologies. This flexibility allows you to manage resources consistently across different cloud platforms. Resource Dependency Management : Terraform intelligently manages dependencies between resources. It automatically determines the order of resource creation, modification, and destruction based on their relationships, ensuring the integrity and consistency of your infrastructure. Plan and Preview Changes : Terraform provides a plan command that allows you to preview the changes that will be made to your infrastructure before applying them. This helps minimize human error and allows you to validate changes and ensure they align with your expectations. Scalability and Modularity : With Terraform, you can create reusable infrastructure modules that encapsulate and abstract common infrastructure patterns. This modularity promotes consistency, reduces duplication, and simplifies the management of complex infrastructure setups.","title":"Why Use Terraform?"},{"location":"level2/terraform/intro/#getting-started-with-terraform","text":"To begin your Terraform journey, you will need to install Terraform on your local machine. Follow the official Terraform installation guide to set up Terraform on your preferred operating system. In the upcoming chapters, we will explore various Terraform concepts and dive deeper into its usage with practical examples and hands-on exercises. Stay tuned and get ready to unlock the power of infrastructure as code with Terraform!","title":"Getting Started with Terraform"},{"location":"level2/terraform/old-plan/","text":"Terraform Curriculum Introduction to Infrastructure as Code (IaC) Basics Overview of Infrastructure as Code (IaC) and its benefits Introduction to declarative vs. imperative approaches in infrastructure provisioning Understanding the role of Terraform in the IaC ecosystem Terraform State Introduction to Terraform state and its importance in managing infrastructure Local state vs. remote state and their pros and cons State management best practices and considerations Understanding the structure and contents of Terraform state files Terraform Backend & State Locking Exploring different backend options for Terraform (local, S3, Azure Storage, etc.) Configuring remote backend for collaborative infrastructure management State locking mechanisms and strategies to prevent concurrent changes Implementing state locking with tools like Terraform Cloud or Consul Variables and Modules Utilizing variables in Terraform to make configurations more flexible and reusable Input variables: Defining variables in Terraform to accept user input Output variables: Extracting and using values from Terraform resources Locals: Defining local variables within Terraform configurations Leveraging Terraform modules for encapsulating reusable infrastructure components Creating and structuring modules for different use cases Module composition and reusability Module versioning and best practices for module development and sharing Using Terraform with Common Providers Hands-on experience with popular cloud providers (AWS, Azure, GCP) in Terraform Provisioning and managing resources using Terraform provider blocks Provider-specific configurations and features AWS provider: Creating EC2 instances, managing S3 buckets, configuring VPCs, etc. Azure provider: Deploying virtual machines, configuring storage accounts, etc. GCP provider: Provisioning Google Compute Engine instances, managing Cloud Storage, etc. HashiCorp Consul provider: Leveraging service discovery and key-value storage Terraform Commands Understanding and utilizing essential Terraform commands: terraform init : Initializing a Terraform working directory terraform plan : Generating an execution plan for infrastructure changes terraform apply : Applying planned changes to provision or modify infrastructure terraform fmt : Formatting Terraform configuration files terraform validate : Validating Terraform configuration files for syntax and best practices terraform destroy : Destroying Terraform-managed infrastructure terraform import : Importing existing resources into Terraform state terraform refresh : Updating Terraform state with the latest resource information Terraform Workflow Step-by-step workflow for managing infrastructure with Terraform Best practices for collaborating, versioning, and managing changes in a team environment Integrating Terraform with Continuous Integration and Continuous Delivery (CI/CD) pipelines Using Terraform in conjunction with tools like Jenkins, GitLab CI/CD, or GitHub Actions Implementing automated Terraform testing and validation in CI/CD pipelines Managing Terraform state and backend configurations in a team setting Creating Your Own Terraform Module Designing and developing a custom Terraform module Structuring modules for reusability and maintainability Implementing module composition in different environments (e.g., production, staging, development) Best practices for documenting, versioning, and sharing modules Publishing modules to the Terraform Registry for broader community use Project: Using Terraform in a Real-World Scenario Practical hands-on project using Terraform to provision infrastructure for a specific use case Applying Terraform best practices and techniques learned throughout the curriculum Managing and deploying infrastructure across multiple environments (production, staging, development) Implementing infrastructure as code for a multi-tier application architecture Conclusion and Next Steps Recap of the Terraform concepts covered in the curriculum Suggestions for further learning and exploration of advanced Terraform topics Advanced Terraform configuration techniques (count, conditional expressions, dynamic blocks) Terraform ecosystem and community tools (Terragrunt, Atlantis, Sentinel) Infrastructure testing and continuous compliance with Terraform Advanced infrastructure deployment patterns with Terraform (blue-green, canary, immutable)","title":"Terraform Curriculum"},{"location":"level2/terraform/old-plan/#terraform-curriculum","text":"","title":"Terraform Curriculum"},{"location":"level2/terraform/old-plan/#introduction-to-infrastructure-as-code-iac-basics","text":"Overview of Infrastructure as Code (IaC) and its benefits Introduction to declarative vs. imperative approaches in infrastructure provisioning Understanding the role of Terraform in the IaC ecosystem","title":"Introduction to Infrastructure as Code (IaC) Basics"},{"location":"level2/terraform/old-plan/#terraform-state","text":"Introduction to Terraform state and its importance in managing infrastructure Local state vs. remote state and their pros and cons State management best practices and considerations Understanding the structure and contents of Terraform state files","title":"Terraform State"},{"location":"level2/terraform/old-plan/#terraform-backend-state-locking","text":"Exploring different backend options for Terraform (local, S3, Azure Storage, etc.) Configuring remote backend for collaborative infrastructure management State locking mechanisms and strategies to prevent concurrent changes Implementing state locking with tools like Terraform Cloud or Consul","title":"Terraform Backend &amp; State Locking"},{"location":"level2/terraform/old-plan/#variables-and-modules","text":"Utilizing variables in Terraform to make configurations more flexible and reusable Input variables: Defining variables in Terraform to accept user input Output variables: Extracting and using values from Terraform resources Locals: Defining local variables within Terraform configurations Leveraging Terraform modules for encapsulating reusable infrastructure components Creating and structuring modules for different use cases Module composition and reusability Module versioning and best practices for module development and sharing","title":"Variables and Modules"},{"location":"level2/terraform/old-plan/#using-terraform-with-common-providers","text":"Hands-on experience with popular cloud providers (AWS, Azure, GCP) in Terraform Provisioning and managing resources using Terraform provider blocks Provider-specific configurations and features AWS provider: Creating EC2 instances, managing S3 buckets, configuring VPCs, etc. Azure provider: Deploying virtual machines, configuring storage accounts, etc. GCP provider: Provisioning Google Compute Engine instances, managing Cloud Storage, etc. HashiCorp Consul provider: Leveraging service discovery and key-value storage","title":"Using Terraform with Common Providers"},{"location":"level2/terraform/old-plan/#terraform-commands","text":"Understanding and utilizing essential Terraform commands: terraform init : Initializing a Terraform working directory terraform plan : Generating an execution plan for infrastructure changes terraform apply : Applying planned changes to provision or modify infrastructure terraform fmt : Formatting Terraform configuration files terraform validate : Validating Terraform configuration files for syntax and best practices terraform destroy : Destroying Terraform-managed infrastructure terraform import : Importing existing resources into Terraform state terraform refresh : Updating Terraform state with the latest resource information","title":"Terraform Commands"},{"location":"level2/terraform/old-plan/#terraform-workflow","text":"Step-by-step workflow for managing infrastructure with Terraform Best practices for collaborating, versioning, and managing changes in a team environment Integrating Terraform with Continuous Integration and Continuous Delivery (CI/CD) pipelines Using Terraform in conjunction with tools like Jenkins, GitLab CI/CD, or GitHub Actions Implementing automated Terraform testing and validation in CI/CD pipelines Managing Terraform state and backend configurations in a team setting","title":"Terraform Workflow"},{"location":"level2/terraform/old-plan/#creating-your-own-terraform-module","text":"Designing and developing a custom Terraform module Structuring modules for reusability and maintainability Implementing module composition in different environments (e.g., production, staging, development) Best practices for documenting, versioning, and sharing modules Publishing modules to the Terraform Registry for broader community use","title":"Creating Your Own Terraform Module"},{"location":"level2/terraform/old-plan/#project-using-terraform-in-a-real-world-scenario","text":"Practical hands-on project using Terraform to provision infrastructure for a specific use case Applying Terraform best practices and techniques learned throughout the curriculum Managing and deploying infrastructure across multiple environments (production, staging, development) Implementing infrastructure as code for a multi-tier application architecture","title":"Project: Using Terraform in a Real-World Scenario"},{"location":"level2/terraform/old-plan/#conclusion-and-next-steps","text":"Recap of the Terraform concepts covered in the curriculum Suggestions for further learning and exploration of advanced Terraform topics Advanced Terraform configuration techniques (count, conditional expressions, dynamic blocks) Terraform ecosystem and community tools (Terragrunt, Atlantis, Sentinel) Infrastructure testing and continuous compliance with Terraform Advanced infrastructure deployment patterns with Terraform (blue-green, canary, immutable)","title":"Conclusion and Next Steps"},{"location":"level2/terraform/plan/","text":"Terraform Syllabus for DevOps Beginners \ud83c\udf31 Module 1: Introduction to Terraform and IaC (Infrastructure as Code) Overview of DevOps and IaC Introduction to Infrastructure as Code (IaC). What is Terraform? History and evolution of Terraform. Terraform vs. other IaC tools. Terraform's Place in the DevOps Lifecycle Understanding where Terraform fits in the CI/CD pipeline. Module 2: Terraform Basics Installation and Setup Installing Terraform. Setting up your first Terraform project. Terraform CLI Basic commands: init , plan , apply , destroy . Understanding the Terraform workflow. Basic Configuration Files Writing your first Terraform configuration. Understanding HCL (HashiCorp Configuration Language). Resource and provider basics. Module 3: Building with Terraform Resource Management Creating and managing resources. Understanding resource dependencies. Variables and Outputs Using input variables. Defining and using output variables. Terraform State Management Understanding Terraform state. Remote state management. Module 4: Modules and Advanced Concepts Modules in Terraform Creating and using modules. Module sources and versioning. Working with Multiple Environments Managing environments (dev, staging, production). Best practices for environment separation. Advanced Resource Features Resource targeting and lifecycle management. Using provisioners. Module 5: Terraform in the Cloud Cloud Providers and Terraform Overview of AWS, Azure, GCP with Terraform. Setting up cloud providers in Terraform. Building a Basic Cloud Infrastructure Creating a simple cloud architecture (e.g., VPC, EC2, S3 on AWS). Managing cloud resources with Terraform. Module 6: Best Practices and Real-world Applications Terraform Best Practices Code organization. Version control with Terraform. Security best practices. Case Studies and Practical Scenarios Real-world use cases. Common pitfalls and how to avoid them. Future Trends and Community Resources Keeping up with Terraform updates. Community resources and continuing education.","title":"Syllabus"},{"location":"level2/terraform/plan/#terraform-syllabus-for-devops-beginners","text":"","title":"Terraform Syllabus for DevOps Beginners \ud83c\udf31"},{"location":"level2/terraform/plan/#module-1-introduction-to-terraform-and-iac-infrastructure-as-code","text":"Overview of DevOps and IaC Introduction to Infrastructure as Code (IaC). What is Terraform? History and evolution of Terraform. Terraform vs. other IaC tools. Terraform's Place in the DevOps Lifecycle Understanding where Terraform fits in the CI/CD pipeline.","title":"Module 1: Introduction to Terraform and IaC (Infrastructure as Code)"},{"location":"level2/terraform/plan/#module-2-terraform-basics","text":"Installation and Setup Installing Terraform. Setting up your first Terraform project. Terraform CLI Basic commands: init , plan , apply , destroy . Understanding the Terraform workflow. Basic Configuration Files Writing your first Terraform configuration. Understanding HCL (HashiCorp Configuration Language). Resource and provider basics.","title":"Module 2: Terraform Basics"},{"location":"level2/terraform/plan/#module-3-building-with-terraform","text":"Resource Management Creating and managing resources. Understanding resource dependencies. Variables and Outputs Using input variables. Defining and using output variables. Terraform State Management Understanding Terraform state. Remote state management.","title":"Module 3: Building with Terraform"},{"location":"level2/terraform/plan/#module-4-modules-and-advanced-concepts","text":"Modules in Terraform Creating and using modules. Module sources and versioning. Working with Multiple Environments Managing environments (dev, staging, production). Best practices for environment separation. Advanced Resource Features Resource targeting and lifecycle management. Using provisioners.","title":"Module 4: Modules and Advanced Concepts"},{"location":"level2/terraform/plan/#module-5-terraform-in-the-cloud","text":"Cloud Providers and Terraform Overview of AWS, Azure, GCP with Terraform. Setting up cloud providers in Terraform. Building a Basic Cloud Infrastructure Creating a simple cloud architecture (e.g., VPC, EC2, S3 on AWS). Managing cloud resources with Terraform.","title":"Module 5: Terraform in the Cloud"},{"location":"level2/terraform/plan/#module-6-best-practices-and-real-world-applications","text":"Terraform Best Practices Code organization. Version control with Terraform. Security best practices. Case Studies and Practical Scenarios Real-world use cases. Common pitfalls and how to avoid them. Future Trends and Community Resources Keeping up with Terraform updates. Community resources and continuing education.","title":"Module 6: Best Practices and Real-world Applications"},{"location":"level3/bash-scripting/basics/","text":"","title":"Basics"},{"location":"level3/golang/basics/","text":"","title":"Golang basics"},{"location":"level3/python/basics/","text":"","title":"Python plan"},{"location":"level4/ansible/basics/","text":"","title":"Ansible plan"},{"location":"level4/helm/basics/","text":"","title":"Helm basics"},{"location":"level4/monitoring/basics/","text":"","title":"Monitoring basics"},{"location":"level4/sre/basics/","text":"","title":"SRE basics"}]}